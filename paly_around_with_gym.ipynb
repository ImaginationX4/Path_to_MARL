{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "350a17c9-3dc6-48b3-be95-b2b76e33ff61",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path\n",
    "sys.path.append('C:\\\\Users\\\\36541\\\\anaconda3\\\\envs\\\\openai\\\\Lib\\\\site-packages')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "47c32026-b706-42b9-bda9-d69aa2364211",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['D:\\\\360_study_materials\\\\MARL\\\\Path_to_MARL\\\\openAI',\n",
       " 'C:\\\\Users\\\\36541\\\\anaconda3\\\\python311.zip',\n",
       " 'C:\\\\Users\\\\36541\\\\anaconda3\\\\DLLs',\n",
       " 'C:\\\\Users\\\\36541\\\\anaconda3\\\\Lib',\n",
       " 'C:\\\\Users\\\\36541\\\\anaconda3',\n",
       " '',\n",
       " 'C:\\\\Users\\\\36541\\\\anaconda3\\\\Lib\\\\site-packages',\n",
       " 'C:\\\\Users\\\\36541\\\\anaconda3\\\\Lib\\\\site-packages\\\\win32',\n",
       " 'C:\\\\Users\\\\36541\\\\anaconda3\\\\Lib\\\\site-packages\\\\win32\\\\lib',\n",
       " 'C:\\\\Users\\\\36541\\\\anaconda3\\\\Lib\\\\site-packages\\\\Pythonwin',\n",
       " 'C:\\\\Users\\\\36541\\\\anaconda3\\\\envs\\\\openai\\\\Lib\\\\site-packages']"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sys.path"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64048314-73d9-474a-a63d-09b7e4dcac0e",
   "metadata": {},
   "source": [
    "<h2>RL with FrozenLake</h2>\n",
    "<p>Fristly we will use q-learning to control and improve every move of agent</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "87e3fcce-e3b5-4d91-a46a-83f02374b41f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.02759273, 0.05130485, 0.03214128, 0.02951271],\n",
       "       [0.09305312, 0.07173051, 0.01503383, 0.04451518],\n",
       "       [0.0300752 , 0.04431072, 0.02050409, 0.02178363],\n",
       "       [0.04618061, 0.04253456, 0.04181581, 0.00873054],\n",
       "       [0.02305321, 0.06014916, 0.04236924, 0.01012737],\n",
       "       [0.00973318, 0.02093988, 0.01116267, 0.0275529 ],\n",
       "       [0.08054146, 0.06344372, 0.05124157, 0.01594153],\n",
       "       [0.04657376, 0.03667984, 0.05996925, 0.07972934],\n",
       "       [0.05119087, 0.0459588 , 0.00527591, 0.05975778],\n",
       "       [0.06172145, 0.04544333, 0.08692103, 0.02420912],\n",
       "       [0.03606536, 0.05117257, 0.07453123, 0.04611979],\n",
       "       [0.05374448, 0.02356084, 0.0621444 , 0.07672921],\n",
       "       [0.06306768, 0.03071108, 0.04899696, 0.0181361 ],\n",
       "       [0.07615041, 0.09915665, 0.0705638 , 0.03554788],\n",
       "       [0.00931007, 0.01661018, 0.07431083, 0.02133869],\n",
       "       [0.07476326, 0.02660031, 0.04811081, 0.05644341]])"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "from IPython.display import clear_output\n",
    "env= gym.make(\"FrozenLake-v1\",render_mode=\"human\")#env\n",
    "#create q-table\n",
    "state_size = env.observation_space.n\n",
    "action_size = env.action_space.n\n",
    "q_table = 0.1*np.random.random([state_size, action_size])\n",
    "q_table"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf712699-be9f-4aea-ac97-5185878817c9",
   "metadata": {},
   "source": [
    "<p>Now we need to train this:</p>\n",
    "<ol>\n",
    "    <li>get the action according to q-table </li>\n",
    "    <li>observe state and rewards </li>\n",
    "    <li>update the q-value in q-table </li>\n",
    "</ol>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f13ab939-47a8-4a7c-869a-e084e004c356",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.57868612 0.17063301 0.57816984 0.56492707]\n",
      " [0.56241247 0.7611826  0.74216403 0.02544439]\n",
      " [0.69139307 0.68942938 0.14163453 0.67544662]\n",
      " [0.45983983 0.13149908 0.58262424 0.67535692]\n",
      " [0.18806529 0.06205141 0.60543356 0.04992693]\n",
      " [0.95742754 0.51994768 0.01063152 0.66318695]\n",
      " [0.7746028  0.20950859 0.7528882  0.1214022 ]\n",
      " [0.30026485 0.20718593 0.63559762 0.60472507]\n",
      " [0.24804659 0.06865012 0.58080306 0.57502082]\n",
      " [0.06095895 0.49095874 0.51295914 0.34032159]\n",
      " [0.60754184 0.66020999 0.64469276 0.28783784]\n",
      " [0.14555165 0.27118272 0.38254015 0.59337237]\n",
      " [0.05467195 0.34512906 0.167713   0.32681001]\n",
      " [0.13351706 0.08148994 0.27467398 0.01590998]\n",
      " [0.50940107 0.72243015 0.74240629 0.2200136 ]\n",
      " [0.7586152  0.60253639 0.25563786 0.89016835]]\n"
     ]
    }
   ],
   "source": [
    "env= gym.make(\"FrozenLake-v1\",render_mode=\"human\")\n",
    "observation, info = env.reset(seed=42)\n",
    "q_table = np.random.random([state_size, action_size])\n",
    "for _ in range(100):\n",
    "    #1\n",
    "    action = np.argmax(q_table[observation])\n",
    "    observation_now=observation\n",
    "    #2\n",
    "    observation, reward, terminated, truncated, info = env.step(action)\n",
    "    #3\n",
    "    q_table[observation_now][action]+=0.1*(reward+0.7*np.max(q_table[observation])-q_table[observation_now][action])\n",
    "    print(q_table)\n",
    "    clear_output(wait=True)\n",
    "    if terminated or truncated:\n",
    "        observation, info = env.reset()\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caf10217-9a40-4d37-85c2-701026a89aba",
   "metadata": {},
   "source": [
    "<p>Oh,man,it doesn't improve themselves too much, what to do????</p>\n",
    "<p>I think we can try to make some exploration by using &epsilon;-greedy</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "24eb7864-e20c-47a5-bf73-9428a47f8919",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "epsilon = 1.0001  \n",
    "epsilon_decay_rate = 0.0001  \n",
    "def get_action(observation,q_table,env):\n",
    "    global epsilon\n",
    "    epsilon-= 0.0001  \n",
    "    action_greedy = np.argmax(q_table[observation])\n",
    "    action_random = env.action_space.sample()\n",
    "    return action_random if random.random() < epsilon else action_greedy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9f655753-5e83-4899-8121-8648ca48324e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.531441 0.59049  0.59049  0.531441]\n",
      " [0.531441 0.       0.6561   0.59049 ]\n",
      " [0.59049  0.729    0.59049  0.6561  ]\n",
      " [0.6561   0.       0.59049  0.59049 ]\n",
      " [0.59049  0.6561   0.       0.531441]\n",
      " [0.       0.       0.       0.      ]\n",
      " [0.       0.81     0.       0.6561  ]\n",
      " [0.       0.       0.       0.      ]\n",
      " [0.6561   0.       0.729    0.59049 ]\n",
      " [0.6561   0.81     0.81     0.      ]\n",
      " [0.729    0.9      0.       0.729   ]\n",
      " [0.       0.       0.       0.      ]\n",
      " [0.       0.       0.       0.      ]\n",
      " [0.       0.81     0.9      0.729   ]\n",
      " [0.81     0.9      1.       0.81    ]\n",
      " [0.       0.       0.       0.      ]]\n"
     ]
    }
   ],
   "source": [
    "env= gym.make(\"FrozenLake-v1\",is_slippery=False,render_mode=\"human\")\n",
    "observation, info = env.reset(seed=42)\n",
    "state_size = env.observation_space.n\n",
    "action_size = env.action_space.n\n",
    "q_table = np.zeros([state_size, action_size])\n",
    "learning_rate_a = 0.9\n",
    "discount_factor_g = 0.9 \n",
    "for _ in range(10000):\n",
    "    #1\n",
    "    action = get_action(observation,q_table,env)\n",
    "    observation_now = observation\n",
    "    #2\n",
    "    observation, reward, terminated, truncated, info = env.step(action)\n",
    "    #3\n",
    "    q_table[observation_now][action]+=learning_rate_a*(reward+learning_rate_a*np.max(q_table[observation])-q_table[observation_now][action])\n",
    "    print(q_table)\n",
    "    clear_output(wait=True)\n",
    "    if terminated or truncated:\n",
    "        observation, info = env.reset()\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "cc228f8f-0ce1-45fb-bf1e-bdc1142b11b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "env= gym.make(\"FrozenLake-v1\",is_slippery=False,render_mode=\"human\")\n",
    "observation, info = env.reset(seed=42)\n",
    "for _ in range(30):\n",
    "    #1\n",
    "    action = get_action(observation,q_table,env)\n",
    "    \n",
    "    observation_now = observation\n",
    "    #2\n",
    "    observation, reward, terminated, truncated, info = env.step(action)\n",
    "    if terminated or truncated:\n",
    "        observation, info = env.reset()\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5e03c7e-c9fc-4113-aee2-439dc9278699",
   "metadata": {},
   "source": [
    "<p>Result looks great, q-learning theory does work!</p>\n",
    "<p>I found that the initial setting of the Q-table is important; it can change the update direction of the whole process >However,the update rate is slow.</p>\r\n",
    "<p>I think we can try using neural networks to see if we can change that. We are going to use the famous paper \"<strong>Human-level control through deep reinforcement learning</strong>\" as our algorithm.</\n",
    "<p>But first let's start with a simple neural network</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3596a04a-3a50-4f8b-8f74-210a97087daf",
   "metadata": {},
   "source": [
    "<h2>simple neural networks</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a2bb42a1-11b8-430c-83be-2a6bb56541b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "class QNetwork(nn.Module):\n",
    "    def __init__(self, state_size, action_size):\n",
    "        super().__init__()\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        self.layer_int = nn.Linear(state_size+action_size, 64)\n",
    "        self.layer_out = nn.Linear(64, 1)\n",
    "    def forward(self, inputs):\n",
    "        q_values = self.layer_int(inputs)\n",
    "        q_values = self.layer_out(q_values)\n",
    "        #action = torch.argmax(q_values).item()\n",
    "        return q_values#.detach().numpy()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "28ef2b2f-cd95-428c-b448-50340323d78a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#one-hot encoding\n",
    "def state_to_dqn_input( state:int, num_states:int,action:int,num_action:int)->torch.Tensor:\n",
    "        input_tensor = torch.zeros(num_states+num_action)\n",
    "        input_tensor[state] = 1\n",
    "        input_tensor[num_states+action] = 1\n",
    "        return input_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "eee310a6-4db1-40ae-8bd7-0669f4751561",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 1.])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#check model's feature\n",
    "model = QNetwork(16,4)\n",
    "observation_now=1\n",
    "action=3\n",
    "one_hot_intputs = state_to_dqn_input(observation_now,16,action,4)\n",
    "one_hot_intputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "cbe94378-be4f-40cf-8796-e192fb64bed2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 3.68488133e-01  3.98570210e-01  3.74067187e-01  4.11178410e-01]\n",
      " [ 2.85750031e-02  4.40580338e-01  4.51021135e-01  3.61900687e-01]\n",
      " [ 5.16256452e-01  3.78689378e-01  5.26460767e-01  3.53765815e-01]\n",
      " [ 3.80863070e-01  1.74489021e-02  2.26877064e-01  3.37339193e-01]\n",
      " [ 2.83553153e-01  4.80325937e-01  3.86109769e-01  2.86018074e-01]\n",
      " [ 0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00]\n",
      " [ 4.82944548e-02  1.85503811e-03  4.64841962e-01  3.68379056e-02]\n",
      " [ 0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00]\n",
      " [ 3.58521521e-01  1.66386453e-05  4.01314318e-01  9.69863776e-03]\n",
      " [ 2.84698904e-02  6.88004494e-01  4.32265878e-01  6.91982806e-01]\n",
      " [ 4.69033539e-01  4.07519996e-01  7.91693568e-01  6.07887506e-02]\n",
      " [ 0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00]\n",
      " [ 0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00]\n",
      " [ 3.89754325e-01  3.58977914e-02  4.28396404e-01 -6.01392612e-03]\n",
      " [ 6.53937459e-01  7.82589078e-01  9.00001884e-01  6.98756397e-01]\n",
      " [ 0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00]]\n"
     ]
    }
   ],
   "source": [
    "env= gym.make(\"FrozenLake-v1\",render_mode=\"human\")\n",
    "observation, info = env.reset(seed=42)\n",
    "state_size = env.observation_space.n\n",
    "action_size = env.action_space.n\n",
    "q_table = np.zeros([state_size, action_size])\n",
    "learning_rate_a = 0.9\n",
    "discount_factor_g = 0.9\n",
    "loss_fn = nn.MSELoss() \n",
    "model = QNetwork(16,4)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "for _ in range(1000):\n",
    "    #1\n",
    "    epsilon = 1\n",
    "    action_greedy = np.argmax(q_table[observation])\n",
    "    action_random = env.action_space.sample()\n",
    "    action = action_random if random.random() < epsilon else action_greedy\n",
    "    epsilon-= 0.001  \n",
    "    observation_now = observation\n",
    "    #2\n",
    "    observation, reward, terminated, truncated, info = env.step(action)\n",
    "    #3\n",
    "    one_hot_intputs = state_to_dqn_input(observation_now,16,action,4)\n",
    "    model.train()\n",
    "    q_value=model(one_hot_intputs)    \n",
    "    q_target= q_value+\\\n",
    "                  learning_rate_a*(reward+discount_factor_g*(1-terminated)*np.max(q_table[observation])-q_value)\n",
    "    \n",
    "    q_table[observation_now][action]=q_target\n",
    "    q_target= torch.tensor([q_target],dtype=torch.float32)\n",
    "    loss= loss_fn(q_value,q_target)\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    print(q_table)\n",
    "    clear_output(wait=True)\n",
    "    if terminated or truncated:\n",
    "        observation, info = env.reset()\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "15410cf6-5576-4bf5-8212-370e1caff01b",
   "metadata": {},
   "outputs": [],
   "source": [
    "env= gym.make(\"FrozenLake-v1\",is_slippery=False,render_mode=\"human\")\n",
    "observation, info = env.reset(seed=42)\n",
    "for _ in range(30):\n",
    "    #1\n",
    "    action = get_action(observation,q_table,env)\n",
    "    \n",
    "    observation_now = observation\n",
    "    #2\n",
    "    observation, reward, terminated, truncated, info = env.step(action)\n",
    "    if terminated or truncated:\n",
    "        observation, info = env.reset()\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2babea54-53cd-4783-ab9d-13ff9f637ed1",
   "metadata": {},
   "source": [
    "<p>Ehhh.... What? it is even wrose!</p>\n",
    "<p>I try to use a neural netwerk to predict q-value, and use it to update the q-table, but it obviously doesn't work out</p>\n",
    "<p>I'd say let's check what the paper said about the netwerk model</p>\n",
    "It says we will use double netwerks; one for predict q-value, other for target value(compare with the frist one)\n",
    "And also experience replay is used for efficiency"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1569ad88-7f15-4c06-8b13-7acad9167927",
   "metadata": {},
   "source": [
    "<h2>Human-level control through deep reinforcement \n",
    "learning</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "48acce56-fa96-47b0-a50d-d08ed9f281f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "from collections import deque\n",
    "import random\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class DQN(nn.Module):\n",
    "    def __init__(self, state_size, action_size):\n",
    "        super().__init__()\n",
    "        self.layer_int = nn.Linear(state_size, 64)\n",
    "        self.layer_out = nn.Linear(64, action_size)\n",
    "    def forward(self, inputs):\n",
    "        q_values = self.layer_int(inputs)\n",
    "        q_values = self.layer_out(q_values)\n",
    "        #action = torch.argmax(q_values).item()\n",
    "        return q_values#.detach().numpy()\n",
    "def state_to_dqn_input( state:int, num_states:int)->torch.Tensor:\n",
    "        input_tensor = torch.zeros(num_states)\n",
    "        input_tensor[state] = 1\n",
    "        return input_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "ebf8d7b3-b5b4-4c86-85a0-112c856ab2c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.1352],\n",
      "        [-0.1024],\n",
      "        [-0.2146]], grad_fn=<GatherBackward0>)\n"
     ]
    }
   ],
   "source": [
    "observation = state_to_dqn_input(1, 16)\n",
    "new_observation = state_to_dqn_input(2, 16)\n",
    "new_observation_1 = state_to_dqn_input(3, 16)\n",
    "a= np.asarray([observation,new_observation,new_observation_1])\n",
    "a= torch.as_tensor(a,dtype=torch.float32)\n",
    "action=torch.tensor([1,2, 3])\n",
    "model = DQN(16,4)\n",
    "b=model(a)\n",
    "print(b.gather(dim=-1,index=action.view(-1,1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ac215e0-0e09-40fd-b728-f13c4fd82b9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#parameter\n",
    "EPOCH=1000\n",
    "learning_rate_a = 0.001         \n",
    "discount_factor_g = 0.9        \n",
    "network_sync_rate = 10         \n",
    "replay_memory_size = 1000      \n",
    "mini_batch_size = 32\n",
    "epsilon = 1\n",
    "#enviorment\n",
    "env= gym.make(\"FrozenLake-v1\",is_slippery=False,render_mode=\"human\")\n",
    "#experience buffer (s_i,a,r,s_i+1,done)\n",
    "memory = deque([], maxlen=100)\n",
    "#double model,loss,optimizer\n",
    "loss_fn = nn.MSELoss()\n",
    "train_model = DQN(16,4)\n",
    "target_model = DQN(16,4)\n",
    "target_model.load_state_dict(train_model.state_dict())\n",
    "optimizer  = torch.optim.Adam(train_model.parameters(), lr=learning_rate_a)\n",
    "#epsilon is combined with epochs\n",
    "epochs = np.linspace(0,1,EPOCH)\n",
    "for i in range(EPOCH):\n",
    "    observation, info = env.reset(seed=42)\n",
    "    terminated = False\n",
    "    truncated  = False\n",
    "    reward_count = 0\n",
    "    while(not reward_count>=40):\n",
    "        if random.random() < epsilon:\n",
    "            # select random action\n",
    "            action = env.action_space.sample() # actions: 0=left,1=down,2=right,3=up\n",
    "        else:         \n",
    "            with torch.no_grad():\n",
    "                action = train_model(state_to_dqn_input(observation, 16)).argmax().item()\n",
    "        # add to memory\n",
    "        new_observation, reward, terminated, truncated, info = env.step(action)\n",
    "        observation_t = state_to_dqn_input(observation, 16)\n",
    "        new_observation_t = state_to_dqn_input(new_observation, 16)\n",
    "        memory.append((observation_t, action , reward, new_observation_t,terminated)) \n",
    "        observation = new_observation\n",
    "        reward_count+=1#reward\n",
    "        if terminated or truncated:\n",
    "            observation, info = env.reset()\n",
    "    if i%100==0:\n",
    "        print('got the gift!'+f'{i} times')\n",
    "    if len(memory)>mini_batch_size:\n",
    "        mini_barch = random.sample(memory, mini_batch_size)\n",
    "        \n",
    "        obs_a = np.asarray([t[0] for t in mini_barch])\n",
    "        action_a = np.asarray([t[1] for t in mini_barch])\n",
    "        reward_a = np.asarray([t[2] for t in mini_barch])\n",
    "        new_obs_a = np.asarray([t[3] for t in mini_barch])\n",
    "        terminated_a = np.asarray([t[4] for t in mini_barch])\n",
    "        \n",
    "        obs_t = torch.as_tensor(obs_a,dtype=torch.float32)\n",
    "        action_t = torch.as_tensor(action_a,dtype=torch.int64).view(-1,1)\n",
    "        reward_t = torch.as_tensor(reward_a,dtype=torch.float32).view(-1,1)\n",
    "        new_obs_t = torch.as_tensor(new_obs_a,dtype=torch.float32)\n",
    "        terminated_t = torch.as_tensor(terminated_a,dtype=torch.float32).view(-1,1)\n",
    "        #compute the target\n",
    "        max_q_next_action = target_model(new_obs_t).max(dim=-1,keepdim=True)[0]\n",
    "        target = reward_t+ (1-terminated_t)*discount_factor_g*max_q_next_action\n",
    "        #compute the loss\n",
    "        q_values = train_model(obs_t).gather(dim=-1,index=action_t)\n",
    "        loss = loss_fn(q_values,target)\n",
    "        #optimizer\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        #update\n",
    "        epsilon = max(epsilon - 1/EPOCH, 0)\n",
    "        if i % network_sync_rate==0:\n",
    "            target_model.load_state_dict(train_model.state_dict())\n",
    "env.close()\n",
    "torch.save(train_model.state_dict(), \"frozen_lake_dql.pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0e61390-fd9c-4473-a7a1-1a38912b1003",
   "metadata": {},
   "source": [
    "<p>CPU is way to slow to compute, i'd suggest you to use google colab where you can use free GPU</p>\n",
    "<p>so i just train the model, download the state_dict file and load it in here</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "29e1c8b7-9a49-4421-803a-e57678eadb87",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "load_model=DQN(16,4)\n",
    "MODEL_SAVE_PATH='D:\\\\360_study_materials\\\\MARL\\\\Path_to_MARL\\\\openAI\\\\frozen_lake_dql.pt'\n",
    "stae_dict=torch.load(MODEL_SAVE_PATH, map_location=torch.device('cpu'), weights_only=True)\n",
    "load_model.load_state_dict(stae_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "46ad3bd9-e037-43aa-8bfc-66ce97114c66",
   "metadata": {},
   "outputs": [],
   "source": [
    "env= gym.make(\"FrozenLake-v1\",is_slippery=False,render_mode=\"human\")\n",
    "observation, info = env.reset(seed=42)\n",
    "for _ in range(30):\n",
    "    #1\n",
    "    observation_t = state_to_dqn_input(observation, 16)\n",
    "    q_values = load_model(observation_t)\n",
    "    action =torch.argmax(q_values).item()\n",
    "    \n",
    "    #2\n",
    "    observation, reward, terminated, truncated, info = env.step(action)\n",
    "    if terminated or truncated:\n",
    "        observation, info = env.reset()\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc935041-a168-4d51-b574-838eacf0fa26",
   "metadata": {},
   "source": [
    "<p>Wonderful! The reult looks great!</p>\n",
    "<h3>Summarize</h3>\n",
    "<ul>\n",
    "    <li>create a Experience Replay buffer </li>\n",
    "    <li>choose an action according to epsilon-greedy</li>\n",
    "    <li>save the infomation to buffer</li>\n",
    "    <li>feed the batch to netwerk</li>\n",
    "    <li>compute the target q-value</li>\n",
    "    <li>train the netwerk:loss.backward(),optimizer.step()...</li>\n",
    "    <li>update the target netwerk every C steps</li>\n",
    "</ul>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
