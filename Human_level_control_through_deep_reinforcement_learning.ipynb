{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyNI53xBODrFkCgxGx85lxFJ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ImaginationX4/Path_to_MARL/blob/master/Human_level_control_through_deep_reinforcement_learning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install gymnasium[classic-control]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ELXniyhmpS0h",
        "outputId": "4c5c28f9-a474-4d2d-c390-746bdb11f29d"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting gymnasium[classic-control]\n",
            "  Downloading gymnasium-0.29.1-py3-none-any.whl (953 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m953.9/953.9 kB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium[classic-control]) (1.25.2)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium[classic-control]) (2.2.1)\n",
            "Requirement already satisfied: typing-extensions>=4.3.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium[classic-control]) (4.11.0)\n",
            "Collecting farama-notifications>=0.0.1 (from gymnasium[classic-control])\n",
            "  Downloading Farama_Notifications-0.0.4-py3-none-any.whl (2.5 kB)\n",
            "Requirement already satisfied: pygame>=2.1.3 in /usr/local/lib/python3.10/dist-packages (from gymnasium[classic-control]) (2.5.2)\n",
            "Installing collected packages: farama-notifications, gymnasium\n",
            "Successfully installed farama-notifications-0.0.4 gymnasium-0.29.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "D6s7ML9Ko9Ly"
      },
      "outputs": [],
      "source": [
        "import gymnasium as gym\n",
        "import numpy as np\n",
        "from collections import deque\n",
        "import random\n",
        "import torch\n",
        "from torch import nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class DQN(nn.Module):\n",
        "    def __init__(self, state_size, action_size):\n",
        "        super().__init__()\n",
        "        self.layer_int = nn.Linear(state_size, 64)\n",
        "        self.layer_out = nn.Linear(64, action_size)\n",
        "    def forward(self, inputs):\n",
        "        q_values = self.layer_int(inputs)\n",
        "        q_values = self.layer_out(q_values)\n",
        "        #action = torch.argmax(q_values).item()\n",
        "        return q_values#.detach().numpy()\n",
        "def state_to_dqn_input( state:int, num_states:int)->torch.Tensor:\n",
        "        input_tensor = torch.zeros(num_states)\n",
        "        input_tensor[state] = 1\n",
        "        return input_tensor"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Setup device agnostic code\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(f\"Using device: {device}\")\n",
        "model_1=DQN(16,4)\n",
        "# Check model device\n",
        "next(model_1.parameters()).device"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wfGrXzM7q66D",
        "outputId": "447e7239-f986-434d-a3d9-99136343c47f"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "device(type='cpu')"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#parameter\n",
        "EPOCH=10000\n",
        "learning_rate_a = 0.001\n",
        "discount_factor_g = 0.9\n",
        "network_sync_rate = 10\n",
        "mini_batch_size = 32\n",
        "epsilon = 1\n",
        "#enviorment\n",
        "env= gym.make(\"FrozenLake-v1\",is_slippery=False)\n",
        "#experience buffer (s_i,a,r,s_i+1,done)\n",
        "memory = deque([], maxlen=100)\n",
        "terminated = False\n",
        "truncated  = False\n",
        "reward_count = 0\n",
        "observation, info = env.reset(seed=42)\n",
        "while(reward_count==0):\n",
        "    action = env.action_space.sample()\n",
        "    # add to memory\n",
        "    new_observation, reward, terminated, truncated, info = env.step(action)\n",
        "    observation_t = state_to_dqn_input(observation, 16)\n",
        "    new_observation_t = state_to_dqn_input(new_observation, 16)\n",
        "    memory.append((observation_t, action , reward, new_observation_t,terminated))\n",
        "    observation = new_observation\n",
        "    reward_count+=reward\n",
        "    if terminated or truncated:\n",
        "        observation, info = env.reset()\n",
        "#double model,loss,optimizer\n",
        "loss_fn = nn.MSELoss()\n",
        "train_model = DQN(16,4).to(device)\n",
        "target_model = DQN(16,4).to(device)\n",
        "target_model.load_state_dict(train_model.state_dict())\n",
        "optimizer  = torch.optim.Adam(train_model.parameters(), lr=learning_rate_a)\n",
        "#epsilon is combined with epochs\n",
        "epochs = np.linspace(0,1,EPOCH)\n",
        "for i in range(EPOCH):\n",
        "  observation, info = env.reset(seed=42)\n",
        "  terminated = False\n",
        "  truncated  = False\n",
        "  while(not terminated and not truncated):\n",
        "    if random.random() < epsilon:\n",
        "        # select random action\n",
        "        action = env.action_space.sample() # actions: 0=left,1=down,2=right,3=up\n",
        "    else:\n",
        "        with torch.no_grad():\n",
        "            action = train_model(state_to_dqn_input(observation, 16).to(device)).argmax().item()\n",
        "    new_observation, reward, terminated, truncated, info = env.step(action)\n",
        "    observation_t = state_to_dqn_input(observation, 16)\n",
        "    new_observation_t = state_to_dqn_input(new_observation, 16)\n",
        "    observation = new_observation\n",
        "\n",
        "    memory.append((observation_t, action , reward, new_observation_t,terminated))\n",
        "  if i%1000==0:\n",
        "      print('got the gift!'+f'{i} times')\n",
        "\n",
        "  mini_barch = random.sample(memory, mini_batch_size)\n",
        "\n",
        "  obs_a = np.asarray([t[0] for t in mini_barch])\n",
        "  action_a = np.asarray([t[1] for t in mini_barch])\n",
        "  reward_a = np.asarray([t[2] for t in mini_barch])\n",
        "  new_obs_a = np.asarray([t[3] for t in mini_barch])\n",
        "  terminated_a = np.asarray([t[4] for t in mini_barch])\n",
        "\n",
        "  obs_t = torch.as_tensor(obs_a,dtype=torch.float32).to(device)\n",
        "  action_t = torch.as_tensor(action_a,dtype=torch.int64).view(-1,1).to(device)\n",
        "  reward_t = torch.as_tensor(reward_a,dtype=torch.float32).to(device).view(-1,1)\n",
        "  new_obs_t = torch.as_tensor(new_obs_a,dtype=torch.float32).to(device)\n",
        "  terminated_t = torch.as_tensor(terminated_a,dtype=torch.float32).to(device).view(-1,1)\n",
        "  #compute the target\n",
        "  max_q_action = target_model(new_obs_t).max(dim=1,keepdim=True)[0]\n",
        "  target = reward_t+ (1-terminated_t)*discount_factor_g*max_q_action\n",
        "  #compute the loss\n",
        "  #print(action_t)\n",
        "  q_values = train_model(obs_t).gather(dim=-1,index=action_t)\n",
        "  loss = F.smooth_l1_loss(q_values,target)\n",
        "  #optimizer\n",
        "  optimizer.zero_grad()\n",
        "  loss.backward()\n",
        "  optimizer.step()\n",
        "  #update\n",
        "  epsilon = max(epsilon - 1/EPOCH, 0)\n",
        "  if i % network_sync_rate==0:\n",
        "      target_model.load_state_dict(train_model.state_dict())\n",
        "env.close()\n",
        "torch.save(train_model.state_dict(), \"frozen_lake_dql.pt\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fuF5DMG9pC3M",
        "outputId": "e5859db1-2b8b-48bc-978e-1d7f9f5bfece"
      },
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "got the gift!0 times\n",
            "got the gift!1000 times\n",
            "got the gift!2000 times\n",
            "got the gift!3000 times\n",
            "got the gift!4000 times\n",
            "got the gift!5000 times\n",
            "got the gift!6000 times\n",
            "got the gift!7000 times\n",
            "got the gift!8000 times\n",
            "got the gift!9000 times\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "a = state_to_dqn_input(2, 16).to(device)\n",
        "train_model(a)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r2lj23E667XF",
        "outputId": "939b8441-894a-4517-994b-ef55a74fd2b0"
      },
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([0.5110, 0.7287, 0.6055, 0.6838], device='cuda:0',\n",
              "       grad_fn=<ViewBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 52
        }
      ]
    }
  ]
}