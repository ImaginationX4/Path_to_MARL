{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a06c1777-75c6-4620-8745-06219309f046",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "100b7da4-224b-4e0e-9b60-62bbe9b3806e",
   "metadata": {},
   "source": [
    "### 1. check this env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6f3141c6-4ea5-491c-8d8c-be755dfdc8b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "env= gym.make('Pendulum-v1', g=9.81,render_mode=\"human\")#env\n",
    "observation, info = env.reset(seed=42)\n",
    "for _ in range(100):\n",
    "    action = env.action_space.sample()\n",
    "    observation, reward, terminated, truncated, info = env.step(action)\n",
    "    if terminated or truncated:\n",
    "        observation, info = env.reset()\n",
    "env.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6497ae96-cdda-4731-b928-c91909c6fdfe",
   "metadata": {},
   "source": [
    "### 2.noise function(copy from [openai baselines](https://github.com/openai/baselines/blob/master/baselines/ddpg/noise.py))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3ba06e60-9159-4496-a800-3e6ca431ddb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class OrnsteinUhlenbeckActionNoise():\n",
    "    def __init__(self, mu, sigma=0.15, theta=.15, dt=1e-2, x0=None):\n",
    "        self.theta = theta\n",
    "        self.mu = mu\n",
    "        self.sigma = sigma\n",
    "        self.dt = dt\n",
    "        self.x0 = x0\n",
    "        self.reset()\n",
    "\n",
    "    def __call__(self):\n",
    "        x = self.x_prev + self.theta * (self.mu - self.x_prev) * self.dt + self.sigma * np.sqrt(self.dt) * np.random.normal(size=self.mu.shape)\n",
    "        self.x_prev = x\n",
    "        return x\n",
    "\n",
    "    def reset(self):\n",
    "        self.x_prev = self.x0 if self.x0 is not None else np.zeros_like(self.mu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c129ce89-9d46-4006-a780-acce43378065",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.01968621,  0.02394039,  0.01288836, -0.01493496])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a= OrnsteinUhlenbeckActionNoise(mu=np.zeros(4))\n",
    "a()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c681d7fa-5fad-4cb1-9c3e-abddb9b9ff61",
   "metadata": {},
   "source": [
    "### 3.repaly buffer(st, at, rt, st+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "23c5235a-de76-4bd2-9bd0-3cf31a661171",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayBuffer():\n",
    "    def __init__(self, size, st_shape,simpe_size):\n",
    "        self.size= size\n",
    "        self.simpe_size=simpe_size\n",
    "        self.count = 0\n",
    "        self.states = np.zeros((size,*st_shape))\n",
    "        self.ations = np.zeros((size,1))\n",
    "        self.rewards = np.zeros((size,1))\n",
    "        self.states_next = np.zeros((size,*st_shape))\n",
    "        self.dones = np.zeros((size,1),dtype=np.bool_)\n",
    "         \n",
    "    def add(self,st,at,rt,st_1,done):\n",
    "        self.states[self.count]=st\n",
    "        self.ations[self.count]=at\n",
    "        self.rewards[self.count]=rt\n",
    "        self.states_next[self.count]=st_1\n",
    "        self.dones[self.count]= done\n",
    "        self.count=(self.count+1) % self.size\n",
    "    def simple_buffer(self):\n",
    "        index = np.random.choice(self.size,self.simpe_size)\n",
    "        return self.states[index],self.ations[index],self.rewards[index],self.states_next[index],self.dones[index]\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e69ab709-e7b1-4323-9c25-1e586d1d422d",
   "metadata": {},
   "source": [
    "test,test,test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f38eea6a-243b-4f5a-a1a1-2593f0facd7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "env= gym.make('Pendulum-v1', g=9.81,render_mode=\"human\")#env\n",
    "observation, info = env.reset(seed=42)\n",
    "\n",
    "a_buffer=ReplayBuffer(100,env.observation_space.shape,2)\n",
    "for _ in range(100):\n",
    "    action = env.action_space.sample()\n",
    "    st=observation\n",
    "    observation, reward, terminated, truncated, info = env.step(action)\n",
    "    st_1 =observation\n",
    "    a_buffer.add(st,action,reward,st_1,terminated)\n",
    "    if terminated or truncated:\n",
    "        observation, info = env.reset()\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8554eef6-ebdf-48e6-908d-3692e4fcc4d4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[-0.99419463, -0.10759667,  4.58249998],\n",
       "        [-0.44225168, -0.896891  ,  3.18491173]]),\n",
       " array([[1.75331879],\n",
       "        [0.189982  ]]),\n",
       " array([[-11.30687016],\n",
       "        [ -5.13085385]]),\n",
       " array([[-0.94069541, -0.33925238,  4.76633358],\n",
       "        [-0.32445133, -0.94590241,  2.55352139]]),\n",
       " array([[False],\n",
       "        [False]]))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a_buffer.simple_buffer()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd551a3d-6909-4081-b5a9-3b7a60d8d585",
   "metadata": {},
   "source": [
    "### 4. Critic Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a267ae76-2f22-4a47-9849-988ccf41dcbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "class Critic_net(nn.Module):\n",
    "    def __init__(self, state_size,action_size):\n",
    "        super().__init__()\n",
    "        self.fc1_s = nn.Linear(*state_size,400)\n",
    "        self.b1_s=nn.BatchNorm1d(400)\n",
    "        f1=1/np.sqrt(self.fc1_s.weight.size()[0])\n",
    "        self.fc1_s.weight.data.uniform_(-f1, f1)\n",
    "        self.fc1_s.bias.data.uniform_(-f1, f1)\n",
    "\n",
    "        self.fc2_s = nn.Linear(400,300)\n",
    "        self.b2_s=nn.BatchNorm1d(300)\n",
    "        f2=1/np.sqrt(self.fc2_s.weight.size()[0])\n",
    "        self.fc2_s.weight.data.uniform_(-f2, f2)\n",
    "        self.fc2_s.bias.data.uniform_(-f2, f2)\n",
    "\n",
    "\n",
    "        self.fc2_a = nn.Linear(action_size,300)\n",
    "        self.b2_a=nn.BatchNorm1d(300)\n",
    "        f2_a=1/np.sqrt(self.fc2_a.weight.size()[0])\n",
    "        self.fc2_a.weight.data.uniform_(-f2_a, f2_a)\n",
    "        self.fc2_a.bias.data.uniform_(-f2_a, f2_a)\n",
    "\n",
    "        self.out_layer = nn.Linear(300,1)\n",
    "        f3 = 0.003\n",
    "        self.out_layer.weight.data.uniform_(-f3, f3)\n",
    "        self.out_layer.bias.data.uniform_(-f3, f3)\n",
    "\n",
    "        #self.optimizer = optim.Adam(self.parameter(),lr=0.001)\n",
    "        \n",
    "    def forward(self,state,action):\n",
    "        \n",
    "        state = self.fc1_s(state)\n",
    "        state = self.b1_s(state)\n",
    "        state = F.relu(state)\n",
    "        state = self.fc2_s(state)\n",
    "        state = self.b2_s(state)\n",
    "        \n",
    "\n",
    "        action = self.fc2_a(action)\n",
    "        action = self.b2_a(action)\n",
    "        \n",
    "        state_action = F.relu(torch.add(state,action))\n",
    "        state_action =  F.relu(self.out_layer(state_action))\n",
    "        \n",
    "        return state_action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8cc4624e-9b99-466d-96c7-402eb61a00f4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.0285],\n",
       "        [0.0133],\n",
       "        [0.0018],\n",
       "        [0.0263],\n",
       "        [0.0141],\n",
       "        [0.0091],\n",
       "        [0.0142],\n",
       "        [0.0478],\n",
       "        [0.0000],\n",
       "        [0.0000]], grad_fn=<ReluBackward0>)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a=Critic_net((4,),1)\n",
    "b=torch.randn((10,4))\n",
    "c=torch.ones((10,1))\n",
    "a(b,c)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c724078-041a-49eb-9771-aa18e9235b50",
   "metadata": {},
   "source": [
    "### 5. Actor net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "56689af3-09d7-43a9-8177-46930e240a48",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Actor_net(nn.Module):\n",
    "    def __init__(self, state_size):\n",
    "        super().__init__()\n",
    "        self.fc1_s = nn.Linear(*state_size,400)\n",
    "        self.b1_s=nn.BatchNorm1d(400)\n",
    "        f1=1/np.sqrt(self.fc1_s.weight.size()[0])\n",
    "        self.fc1_s.weight.data.uniform_(-f1, f1)\n",
    "        self.fc1_s.bias.data.uniform_(-f1, f1)\n",
    "\n",
    "        self.fc2_s = nn.Linear(400,300)\n",
    "        self.b2_s=nn.BatchNorm1d(300)\n",
    "        f2=1/np.sqrt(self.fc2_s.weight.size()[0])\n",
    "        self.fc2_s.weight.data.uniform_(-f2, f2)\n",
    "        self.fc2_s.bias.data.uniform_(-f2, f2)\n",
    "\n",
    "        self.out_layer = nn.Linear(300,1)\n",
    "        f3 = 0.0003\n",
    "        self.out_layer.weight.data.uniform_(-f3, f3)\n",
    "        self.out_layer.bias.data.uniform_(-f3, f3)\n",
    "\n",
    "        \n",
    "    def forward(self,state):\n",
    "        state = self.fc1_s(state)\n",
    "        state = self.b1_s(state)\n",
    "        state = F.relu(state)\n",
    "        state = self.fc2_s(state)\n",
    "        state = self.b2_s(state)\n",
    "        state = F.relu(state)\n",
    "        state = self.out_layer(state)\n",
    "        state = nn.Tanh(state)\n",
    "        return state\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5425780-f400-4843-81ee-219d57391cc2",
   "metadata": {},
   "source": [
    "### 6.Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1c650f26-e102-4966-943f-afb1d65ec162",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Randomly initialize critic network Q(s, a|θQ) and actor µ(s|θµ) with weights θ Q and θµ\n",
    "critic = Critic_net(env.observation_space.shape,1)\n",
    "optimizer_c = torch.optim.Adam(critic.parameters(), lr=0.001)\n",
    "actor =  Actor_net(env.observation_space.shape)\n",
    "optimizer_a = torch.optim.Adam(actor.parameters(), lr=0.0001)\n",
    "#Initialize target network\n",
    "target_critic = Critic_net(env.observation_space.shape,1)\n",
    "target_actor =  Actor_net(env.observation_space.shape)\n",
    "#Initialize replay buffer R\n",
    "buffer=ReplayBuffer(1000,env.observation_space.shape,32)#??size,simple_size\n",
    "#for episode = 1, M do\n",
    "epoch =10\n",
    "for i in range(epoch):\n",
    "    #Initialize a random process N for action exploration\n",
    "    noise = OrnsteinUhlenbeckActionNoise(mu=np.zeros(*env.observation_space.shape))\n",
    "    #Receive initial observation state s1\n",
    "    env= gym.make('Pendulum-v1', g=9.81,render_mode=\"human\")#env\n",
    "    observation, info = env.reset(seed=42)\n",
    "    for j in range(1000):\n",
    "        st = observation\n",
    "        actor.eval()\n",
    "        acion = actor(state)\n",
    "        action += torch.tensor(noise(),dtype=torch.float)\n",
    "        actor.train()\n",
    "        observation, reward, terminated, truncated, info = env.step(action)\n",
    "        st_1 =observation\n",
    "        buffer.add(st,action,reward,st_1,terminated)\n",
    "        if i>=1:\n",
    "            #Sample a random minibatch of N transitions (si, ai, ri, si+1) from R\n",
    "            sts,actions,rewards,st_1s,dones=buffer.simple_buffer()\n",
    "            sts = torch.tensor(sts,dytpe=torch.float)\n",
    "            actions = torch.tensor(actions,dytpe=torch.float)\n",
    "            rewards = torch.tensor(rewards,dytpe=torch.float)\n",
    "            st_1s = torch.tensor(st_1s,dytpe=torch.float)\n",
    "            dones = torch.tensor(dones)\n",
    "            #get the loss of critic\n",
    "            target_actions_next =  target_actor(st_1s)\n",
    "            gamma=0.99\n",
    "            target_q = rewards+ gamma*target_critic(st_1s,target_actions_next)*(1-dones)\n",
    "            \n",
    "            q_vlaues = critic(sts,actions)\n",
    "            critic_loss = F.mse_loss(target_q,q_vlaues)\n",
    "            optimizer_c.zero_grad()\n",
    "            critic_loss.backward()\n",
    "            optimizer_c.step()\n",
    "\n",
    "            #get the loss of actor\n",
    "            actor_loss = -critic(sts,actor(sts))\n",
    "            actor_loss = torch.mean(actor_loss)\n",
    "            optimizer_a.zero_grad()\n",
    "            actor_loss.backward()\n",
    "            optimizer_a.step()\n",
    "            \n",
    "            #update target\n",
    "            tau = 0.001\n",
    "            for p_,p in zip(target_critic.parameters(),critic.parameters()):\n",
    "                new_val = tau*p+(1-tau)*p_\n",
    "                p_.copy_(new_val)\n",
    "            for p1_,p1 in zip(target_actor.parameters(),actor.parameters()):\n",
    "                new_val = tau*p1+(1-tau)*p1_\n",
    "                p1_.copy_(new_val)\n",
    "            \n",
    "\n",
    "        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
