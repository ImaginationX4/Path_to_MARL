{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyMYbxAuaaHLS6gisbTNG4ex",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ImaginationX4/Path_to_MARL/blob/master/4_Human_level_control_through_deep_reinforcement_learning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install gymnasium[classic-control]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ELXniyhmpS0h",
        "outputId": "07ef35c6-a240-44de-83c2-8c8302a7b7e8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting gymnasium[classic-control]\n",
            "  Downloading gymnasium-0.29.1-py3-none-any.whl (953 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m953.9/953.9 kB\u001b[0m \u001b[31m7.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium[classic-control]) (1.25.2)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium[classic-control]) (2.2.1)\n",
            "Requirement already satisfied: typing-extensions>=4.3.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium[classic-control]) (4.11.0)\n",
            "Collecting farama-notifications>=0.0.1 (from gymnasium[classic-control])\n",
            "  Downloading Farama_Notifications-0.0.4-py3-none-any.whl (2.5 kB)\n",
            "Requirement already satisfied: pygame>=2.1.3 in /usr/local/lib/python3.10/dist-packages (from gymnasium[classic-control]) (2.5.2)\n",
            "Installing collected packages: farama-notifications, gymnasium\n",
            "Successfully installed farama-notifications-0.0.4 gymnasium-0.29.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Frozen-Lake\n",
        "\n",
        "We use colab tp try our neural netwerk, for more details check the corresponding  jupyter note book.\n"
      ],
      "metadata": {
        "id": "QbGsvAyLs-iz"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D6s7ML9Ko9Ly"
      },
      "outputs": [],
      "source": [
        "import gymnasium as gym\n",
        "import numpy as np\n",
        "from collections import deque\n",
        "import random\n",
        "import torch\n",
        "from torch import nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class DQN(nn.Module):\n",
        "    def __init__(self, state_size, action_size):\n",
        "        super().__init__()\n",
        "        self.layer_int = nn.Linear(state_size, 64)\n",
        "        self.layer_out = nn.Linear(64, action_size)\n",
        "    def forward(self, inputs):\n",
        "        q_values = self.layer_int(inputs)\n",
        "        q_values = self.layer_out(q_values)\n",
        "        #action = torch.argmax(q_values).item()\n",
        "        return q_values#.detach().numpy()\n",
        "def state_to_dqn_input( state:int, num_states:int)->torch.Tensor:\n",
        "        input_tensor = torch.zeros(num_states)\n",
        "        input_tensor[state] = 1\n",
        "        return input_tensor"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Setup device agnostic code\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(f\"Using device: {device}\")\n",
        "model_1=DQN(16,4)\n",
        "# Check model device\n",
        "next(model_1.parameters()).device"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wfGrXzM7q66D",
        "outputId": "3110aeb5-1854-4cdd-92b0-49c95a6cf20f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "device(type='cpu')"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#parameter\n",
        "EPOCH=10000\n",
        "learning_rate_a = 0.001\n",
        "discount_factor_g = 0.9\n",
        "network_sync_rate = 10\n",
        "mini_batch_size = 32\n",
        "epsilon = 1\n",
        "#enviorment\n",
        "env= gym.make(\"FrozenLake-v1\",is_slippery=False)\n",
        "#experience buffer (s_i,a,r,s_i+1,done)\n",
        "memory = deque([], maxlen=100)\n",
        "terminated = False\n",
        "truncated  = False\n",
        "reward_count = 0\n",
        "observation, info = env.reset(seed=42)\n",
        "while(reward_count==0):\n",
        "    action = env.action_space.sample()\n",
        "    # add to memory\n",
        "    new_observation, reward, terminated, truncated, info = env.step(action)\n",
        "    observation_t = state_to_dqn_input(observation, 16)\n",
        "    new_observation_t = state_to_dqn_input(new_observation, 16)\n",
        "    memory.append((observation_t, action , reward, new_observation_t,terminated))\n",
        "    observation = new_observation\n",
        "    reward_count+=reward\n",
        "    if terminated or truncated:\n",
        "        observation, info = env.reset()\n",
        "#double model,loss,optimizer\n",
        "loss_fn = nn.MSELoss()\n",
        "train_model = DQN(16,4).to(device)\n",
        "target_model = DQN(16,4).to(device)\n",
        "target_model.load_state_dict(train_model.state_dict())\n",
        "optimizer  = torch.optim.Adam(train_model.parameters(), lr=learning_rate_a)\n",
        "#epsilon is combined with epochs\n",
        "epochs = np.linspace(0,1,EPOCH)\n",
        "for i in range(EPOCH):\n",
        "  observation, info = env.reset(seed=42)\n",
        "  terminated = False\n",
        "  truncated  = False\n",
        "  while(not terminated and not truncated):\n",
        "    if random.random() < epsilon:\n",
        "        # select random action\n",
        "        action = env.action_space.sample() # actions: 0=left,1=down,2=right,3=up\n",
        "    else:\n",
        "        with torch.no_grad():\n",
        "            action = train_model(state_to_dqn_input(observation, 16).to(device)).argmax().item()\n",
        "    new_observation, reward, terminated, truncated, info = env.step(action)\n",
        "    observation_t = state_to_dqn_input(observation, 16)\n",
        "    new_observation_t = state_to_dqn_input(new_observation, 16)\n",
        "    observation = new_observation\n",
        "\n",
        "    memory.append((observation_t, action , reward, new_observation_t,terminated))\n",
        "  if i%1000==0:\n",
        "      print('got the gift!'+f'{i} times')\n",
        "\n",
        "  mini_barch = random.sample(memory, mini_batch_size)\n",
        "\n",
        "  obs_a = np.asarray([t[0] for t in mini_barch])\n",
        "  action_a = np.asarray([t[1] for t in mini_barch])\n",
        "  reward_a = np.asarray([t[2] for t in mini_barch])\n",
        "  new_obs_a = np.asarray([t[3] for t in mini_barch])\n",
        "  terminated_a = np.asarray([t[4] for t in mini_barch])\n",
        "\n",
        "  obs_t = torch.as_tensor(obs_a,dtype=torch.float32).to(device)\n",
        "  action_t = torch.as_tensor(action_a,dtype=torch.int64).view(-1,1).to(device)\n",
        "  reward_t = torch.as_tensor(reward_a,dtype=torch.float32).to(device).view(-1,1)\n",
        "  new_obs_t = torch.as_tensor(new_obs_a,dtype=torch.float32).to(device)\n",
        "  terminated_t = torch.as_tensor(terminated_a,dtype=torch.float32).to(device).view(-1,1)\n",
        "  #compute the target\n",
        "  max_q_action = target_model(new_obs_t).max(dim=1,keepdim=True)[0]\n",
        "  target = reward_t+ (1-terminated_t)*discount_factor_g*max_q_action\n",
        "  #compute the loss\n",
        "  #print(action_t)\n",
        "  q_values = train_model(obs_t).gather(dim=-1,index=action_t)\n",
        "  loss = F.smooth_l1_loss(q_values,target)\n",
        "  #optimizer\n",
        "  optimizer.zero_grad()\n",
        "  loss.backward()\n",
        "  optimizer.step()\n",
        "  #update\n",
        "  epsilon = max(epsilon - 1/EPOCH, 0)\n",
        "  if i % network_sync_rate==0:\n",
        "      target_model.load_state_dict(train_model.state_dict())\n",
        "env.close()\n",
        "torch.save(train_model.state_dict(), \"frozen_lake_dql.pt\")"
      ],
      "metadata": {
        "id": "fuF5DMG9pC3M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "a = state_to_dqn_input(2, 16).to(device)\n",
        "train_model(a)"
      ],
      "metadata": {
        "id": "r2lj23E667XF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Let's try Cart Pole!\n",
        "\n",
        "Action: left/right 0/1 <br>\n",
        "Observation: Cart Position, Cart Velocity, Pole Angle, Pole Angular Velocity\n",
        "\n"
      ],
      "metadata": {
        "id": "NH0BPjD2sHVe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#netwerk\n",
        "class deep_Q(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.net= nn.Sequential(\n",
        "            nn.Linear(4,64),\n",
        "            nn.Tanh(),\n",
        "            nn.Linear(64,2)\n",
        "        )\n",
        "\n",
        "    def forward(self,x):\n",
        "        return self.net(x)\n",
        "\n"
      ],
      "metadata": {
        "id": "kz2lNIiksb6u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "replay_buffer"
      ],
      "metadata": {
        "id": "Kv50SvdP09h6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "replay_buffer= deque([], maxlen=100)\n",
        "env= gym.make('CartPole-v1')\n",
        "observation, info = env.reset(seed=42)\n",
        "for _ in range(100):\n",
        "    #1\n",
        "    action =env.action_space.sample()\n",
        "\n",
        "    #2\n",
        "\n",
        "    observation_next, reward, terminated, truncated, info = env.step(action)\n",
        "    states =(observation,action,observation_next,reward,terminated)\n",
        "    replay_buffer.append(states)\n",
        "    observation = observation_next\n",
        "    #print(observation_next)\n",
        "\n",
        "    if terminated or truncated:\n",
        "        observation, info = env.reset()\n",
        "env.close()"
      ],
      "metadata": {
        "id": "Ro_0iMb_xs7z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Training"
      ],
      "metadata": {
        "id": "SheBdcng1HiM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "EPOCH =10000\n",
        "learning_rate_a = 0.001\n",
        "discount_factor_g = 0.9\n",
        "network_sync_rate = 10\n",
        "mini_batch_size = 32\n",
        "epsilon = 1\n",
        "train_model = deep_Q().to(device)\n",
        "target_model = deep_Q().to(device)\n",
        "optimizer  = torch.optim.Adam(train_model.parameters(), lr=learning_rate_a)\n",
        "for i in range(EPOCH):\n",
        "  observation, info = env.reset(seed=42)\n",
        "  terminated = False\n",
        "  truncated  = False\n",
        "  while(not terminated or not truncated):\n",
        "    #1\n",
        "    if random.random()<epsilon:\n",
        "      action =env.action_space.sample()\n",
        "    else:\n",
        "      with torch.no_grad():\n",
        "        action = train_model(torch.tensor(observation,dtype=torch.float32).to(device)).argmax().item()\n",
        "    #2\n",
        "    observation_next, reward, terminated, truncated, info = env.step(action)\n",
        "    states =(observation,[action],observation_next,[reward],[terminated])\n",
        "    replay_buffer.append(states)\n",
        "    observation = observation_next\n",
        "\n",
        "  mini_barch = random.sample(replay_buffer, mini_batch_size)\n",
        "  obs_n = np.asarray([t[0] for t in mini_barch])\n",
        "  action_n = np.asarray([t[1] for t in mini_barch])\n",
        "  obs_next_n = np.asarray([t[1] for t in mini_barch])\n",
        "  reward_n = np.asarray([t[1] for t in mini_barch])\n",
        "  terminated_n = np.asarray([t[1] for t in mini_barch])\n",
        "\n",
        "  obs_t = torch.as_tensor(obs_n,dtype=torch.float32).to(device)\n",
        "  action_t = torch.as_tensor(action_n,dtype=torch.int64).to(device)\n",
        "  obs_next_t = torch.as_tensor(obs_next_n,dtype=torch.float32).to(device)\n",
        "  reward_t = torch.as_tensor(reward_n,dtype=torch.float32).to(device)\n",
        "  terminated_t = torch.as_tensor(terminated_n,dtype=torch.float32).to(device)\n",
        "\n",
        "  max_q_values = target_model(obs_t).max(-1,keepdim=True)[0]\n",
        "  target = reward_t + (1-terminated_t)*discount_factor_g*max_q_values\n",
        "  if i % 100==0:\n",
        "    print(f'{i}times')\n",
        "\n",
        "  q_values = train_model(obs_t).gather(dim=-1,index=action_t)\n",
        "  loss = F.smooth_l1_loss(q_values,target)\n",
        "  optimizer.zero_grad()\n",
        "  loss.backward()\n",
        "  optimizer.step()\n",
        "  #update\n",
        "  epsilon = max(epsilon - 1/EPOCH, 0)\n",
        "  if i % network_sync_rate==0:\n",
        "      target_model.load_state_dict(train_model.state_dict())\n",
        "env.close()\n",
        "torch.save(train_model.state_dict(), \"cart_Pole_dql.pt\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E0_qDLd508kb",
        "outputId": "54ae55ac-ce74-4791-9c97-01760ce1e9c4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0times\n",
            "100times\n",
            "200times\n",
            "300times\n",
            "400times\n",
            "500times\n",
            "600times\n",
            "700times\n",
            "800times\n",
            "900times\n",
            "1000times\n",
            "1100times\n",
            "1200times\n",
            "1300times\n",
            "1400times\n",
            "1500times\n",
            "1600times\n",
            "1700times\n",
            "1800times\n",
            "1900times\n",
            "2000times\n",
            "2100times\n",
            "2200times\n",
            "2300times\n",
            "2400times\n",
            "2500times\n",
            "2600times\n",
            "2700times\n",
            "2800times\n",
            "2900times\n",
            "3000times\n",
            "3100times\n",
            "3200times\n",
            "3300times\n",
            "3400times\n",
            "3500times\n",
            "3600times\n",
            "3700times\n",
            "3800times\n",
            "3900times\n",
            "4000times\n",
            "4100times\n",
            "4200times\n",
            "4300times\n",
            "4400times\n",
            "4500times\n",
            "4600times\n",
            "4700times\n",
            "4800times\n",
            "4900times\n",
            "5000times\n",
            "5100times\n",
            "5200times\n",
            "5300times\n",
            "5400times\n",
            "5500times\n",
            "5600times\n",
            "5700times\n",
            "5800times\n",
            "5900times\n",
            "6000times\n",
            "6100times\n",
            "6200times\n",
            "6300times\n",
            "6400times\n",
            "6500times\n",
            "6600times\n",
            "6700times\n",
            "6800times\n",
            "6900times\n",
            "7000times\n",
            "7100times\n",
            "7200times\n",
            "7300times\n",
            "7400times\n",
            "7500times\n",
            "7600times\n",
            "7700times\n",
            "7800times\n",
            "7900times\n",
            "8000times\n",
            "8100times\n",
            "8200times\n",
            "8300times\n",
            "8400times\n",
            "8500times\n",
            "8600times\n",
            "8700times\n",
            "8800times\n",
            "8900times\n",
            "9000times\n",
            "9100times\n",
            "9200times\n",
            "9300times\n",
            "9400times\n",
            "9500times\n",
            "9600times\n",
            "9700times\n",
            "9800times\n",
            "9900times\n"
          ]
        }
      ]
    }
  ]
}