{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "-BxTBETN0PwP",
        "CZwfxp9oD9xO",
        "DSOm1YtuVgET",
        "S6bDrQ6H8g01"
      ],
      "toc_visible": true,
      "gpuType": "T4",
      "authorship_tag": "ABX9TyM1f1DhuS6FQ3RtdfgtqD31",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ImaginationX4/Path_to_MARL/blob/master/Deep_Q_Learning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Dueling Network ⚔️ Model"
      ],
      "metadata": {
        "id": "-BxTBETN0PwP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Model we build has 3 parts:\n",
        "\n",
        "<ol>\n",
        "  <li>common CNN</li>\n",
        "  <li>fully connected layers for Sate/Action-value</li>\n",
        "  <li>output :Q-Value</li>\n",
        "</ol>\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "qRbGOh5V0blw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#credit: https://nn.labml.ai/rl/dqn/model.html\n",
        "import torch\n",
        "from torch import nn\n",
        "\n",
        "class  Model(nn.Module):\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "    #CNN\n",
        "    self.conv = nn.Sequential(\n",
        "            nn.Conv2d(in_channels=4, out_channels=32, kernel_size=8, stride=4),\n",
        "            # W2 = (W1 - F + 2*P) / S + 1\n",
        "            # H2 = (H1 - F + 2*P) / S + 1\n",
        "            nn.ReLU(),\n",
        "\n",
        "            nn.Conv2d(in_channels=32, out_channels=64, kernel_size=4, stride=2),\n",
        "            nn.ReLU(),\n",
        "\n",
        "            nn.Conv2d(in_channels=64, out_channels=64, kernel_size=3, stride=1),\n",
        "            nn.ReLU() #7x7\n",
        "        )\n",
        "    #flatten layer -->512 features\n",
        "    self.lin = nn.Linear(in_features=7 * 7 * 64, out_features=512)\n",
        "    self.activation = nn.ReLU()\n",
        "\n",
        "    #V\n",
        "    self.state_value = nn.Sequential(\n",
        "         nn.Linear(in_features=512, out_features=256),\n",
        "         nn.ReLU(),\n",
        "         nn.Linear(in_features=256, out_features=1),\n",
        "    )\n",
        "    #A\n",
        "    self.action_value = nn.Sequential(\n",
        "         nn.Linear(in_features=512, out_features=256),\n",
        "         nn.ReLU(),\n",
        "         nn.Linear(in_features=256, out_features=4),\n",
        "    )\n",
        "\n",
        "  def forward(self, x: torch.Tensor):\n",
        "    x=self.conv(x)\n",
        "    x=x.reshape((-1, 7 * 7 * 64))\n",
        "    x=self.activation(self.lin(x))\n",
        "    action_value = self.action_value(x)\n",
        "    state_value = self.state_value(x)\n",
        "    action_score_centered = action_value - action_value.mean(dim=-1, keepdim=True)\n",
        "    q = state_value + action_score_centered\n",
        "    return q\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "7M8UXpE30MfO"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "m=Model()\n",
        "input=torch.ones(size=(32,4, 84, 84))\n",
        "output=m(input)\n",
        "output"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BBsYuUIpdGhU",
        "outputId": "b2ebe4b0-7437-4b22-f614-ad17b4f0768c"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[-0.0336,  0.0079,  0.0192, -0.0244],\n",
              "        [-0.0336,  0.0079,  0.0192, -0.0244],\n",
              "        [-0.0336,  0.0079,  0.0192, -0.0244],\n",
              "        [-0.0336,  0.0079,  0.0192, -0.0244],\n",
              "        [-0.0336,  0.0079,  0.0192, -0.0244],\n",
              "        [-0.0336,  0.0079,  0.0192, -0.0244],\n",
              "        [-0.0336,  0.0079,  0.0192, -0.0244],\n",
              "        [-0.0336,  0.0079,  0.0192, -0.0244],\n",
              "        [-0.0336,  0.0079,  0.0192, -0.0244],\n",
              "        [-0.0336,  0.0079,  0.0192, -0.0244],\n",
              "        [-0.0336,  0.0079,  0.0192, -0.0244],\n",
              "        [-0.0336,  0.0079,  0.0192, -0.0244],\n",
              "        [-0.0336,  0.0079,  0.0192, -0.0244],\n",
              "        [-0.0336,  0.0079,  0.0192, -0.0244],\n",
              "        [-0.0336,  0.0079,  0.0192, -0.0244],\n",
              "        [-0.0336,  0.0079,  0.0192, -0.0244],\n",
              "        [-0.0336,  0.0079,  0.0192, -0.0244],\n",
              "        [-0.0336,  0.0079,  0.0192, -0.0244],\n",
              "        [-0.0336,  0.0079,  0.0192, -0.0244],\n",
              "        [-0.0336,  0.0079,  0.0192, -0.0244],\n",
              "        [-0.0336,  0.0079,  0.0192, -0.0244],\n",
              "        [-0.0336,  0.0079,  0.0192, -0.0244],\n",
              "        [-0.0336,  0.0079,  0.0192, -0.0244],\n",
              "        [-0.0336,  0.0079,  0.0192, -0.0244],\n",
              "        [-0.0336,  0.0079,  0.0192, -0.0244],\n",
              "        [-0.0336,  0.0079,  0.0192, -0.0244],\n",
              "        [-0.0336,  0.0079,  0.0192, -0.0244],\n",
              "        [-0.0336,  0.0079,  0.0192, -0.0244],\n",
              "        [-0.0336,  0.0079,  0.0192, -0.0244],\n",
              "        [-0.0336,  0.0079,  0.0192, -0.0244],\n",
              "        [-0.0336,  0.0079,  0.0192, -0.0244],\n",
              "        [-0.0336,  0.0079,  0.0192, -0.0244]], grad_fn=<AddBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Prioritized Experience Replay Buffer"
      ],
      "metadata": {
        "id": "-UFZ_ZY1-D-W"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "[float('inf') for _ in range(2 * 3)]\n",
        "a=2\n",
        "a//= 2#floor\n",
        "a"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_ItHAONnNHby",
        "outputId": "eed959dd-8ac7-427a-9e65-9b2a4c0c1f14"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h2>Compute importance-sampling weight\n",
        "</h2>\n",
        "<p>If you dont know segment tree(Complete Binary Tree/Heap), check <a href='https://www.youtube.com/watch?v=xztU7lmDLv8'>this video</a> on Youtube to gain a better understding</p>\n",
        "<p>In our case we start indexing at 1</p>"
      ],
      "metadata": {
        "id": "gLYzCHsNZKIs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "##Let's understnd some funtions/methods\n",
        "import random\n",
        "import numpy as np\n",
        "#we need to add all info to the Buffer, and what's kind of data structures we use?\n",
        "#Queue!\n",
        "data= {\n",
        "        'obs': np.zeros(shape=(32, 4, 84, 84), dtype=np.uint8),\n",
        "        'action': np.zeros(shape=32, dtype=np.int32),\n",
        "        'reward': np.zeros(shape=32, dtype=np.float32),\n",
        "        'next_obs': np.zeros(shape=(32, 4, 84, 84), dtype=np.uint8),\n",
        "        'done': np.zeros(shape=32, dtype=np.bool_)\n",
        "    }\n",
        "\n",
        "def add(obs, action, reward, next_obs, done,idx,data):\n",
        "  '''\n",
        "  obs is infomation gained from the env to predict the Q-value\n",
        "  done means game is over or not?\n",
        "  idx index\n",
        "  data place to stare\n",
        "  '''\n",
        "  data['obs'][idx] = obs\n",
        "  data['action'][idx] = action\n",
        "  data['reward'][idx] = reward\n",
        "  data['next_obs'][idx] = next_obs\n",
        "  data['done'][idx] = done\n",
        "\n",
        "  next_idx= (idx+1) % data['obs'].shape[0]\n",
        "\n",
        "  return (data['obs'][idx],data['action'][idx],data['reward'][idx],data['next_obs'][idx],data['done'][idx])\n",
        "\n",
        "obs= np.ones(shape=(4, 84, 84), dtype=np.uint8)\n",
        "action= 2\n",
        "reward = -999\n",
        "next_obs = np.ones(shape=(4, 84, 84), dtype=np.uint8)\n",
        "done = False\n",
        "print(add(obs, action, reward, next_obs, done,1,data))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SILSX-aDeh1W",
        "outputId": "5caf03d5-2a90-4b58-ce0b-999ca5485dc9"
      },
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(array([[[1, 1, 1, ..., 1, 1, 1],\n",
            "        [1, 1, 1, ..., 1, 1, 1],\n",
            "        [1, 1, 1, ..., 1, 1, 1],\n",
            "        ...,\n",
            "        [1, 1, 1, ..., 1, 1, 1],\n",
            "        [1, 1, 1, ..., 1, 1, 1],\n",
            "        [1, 1, 1, ..., 1, 1, 1]],\n",
            "\n",
            "       [[1, 1, 1, ..., 1, 1, 1],\n",
            "        [1, 1, 1, ..., 1, 1, 1],\n",
            "        [1, 1, 1, ..., 1, 1, 1],\n",
            "        ...,\n",
            "        [1, 1, 1, ..., 1, 1, 1],\n",
            "        [1, 1, 1, ..., 1, 1, 1],\n",
            "        [1, 1, 1, ..., 1, 1, 1]],\n",
            "\n",
            "       [[1, 1, 1, ..., 1, 1, 1],\n",
            "        [1, 1, 1, ..., 1, 1, 1],\n",
            "        [1, 1, 1, ..., 1, 1, 1],\n",
            "        ...,\n",
            "        [1, 1, 1, ..., 1, 1, 1],\n",
            "        [1, 1, 1, ..., 1, 1, 1],\n",
            "        [1, 1, 1, ..., 1, 1, 1]],\n",
            "\n",
            "       [[1, 1, 1, ..., 1, 1, 1],\n",
            "        [1, 1, 1, ..., 1, 1, 1],\n",
            "        [1, 1, 1, ..., 1, 1, 1],\n",
            "        ...,\n",
            "        [1, 1, 1, ..., 1, 1, 1],\n",
            "        [1, 1, 1, ..., 1, 1, 1],\n",
            "        [1, 1, 1, ..., 1, 1, 1]]], dtype=uint8), 2, -999.0, array([[[1, 1, 1, ..., 1, 1, 1],\n",
            "        [1, 1, 1, ..., 1, 1, 1],\n",
            "        [1, 1, 1, ..., 1, 1, 1],\n",
            "        ...,\n",
            "        [1, 1, 1, ..., 1, 1, 1],\n",
            "        [1, 1, 1, ..., 1, 1, 1],\n",
            "        [1, 1, 1, ..., 1, 1, 1]],\n",
            "\n",
            "       [[1, 1, 1, ..., 1, 1, 1],\n",
            "        [1, 1, 1, ..., 1, 1, 1],\n",
            "        [1, 1, 1, ..., 1, 1, 1],\n",
            "        ...,\n",
            "        [1, 1, 1, ..., 1, 1, 1],\n",
            "        [1, 1, 1, ..., 1, 1, 1],\n",
            "        [1, 1, 1, ..., 1, 1, 1]],\n",
            "\n",
            "       [[1, 1, 1, ..., 1, 1, 1],\n",
            "        [1, 1, 1, ..., 1, 1, 1],\n",
            "        [1, 1, 1, ..., 1, 1, 1],\n",
            "        ...,\n",
            "        [1, 1, 1, ..., 1, 1, 1],\n",
            "        [1, 1, 1, ..., 1, 1, 1],\n",
            "        [1, 1, 1, ..., 1, 1, 1]],\n",
            "\n",
            "       [[1, 1, 1, ..., 1, 1, 1],\n",
            "        [1, 1, 1, ..., 1, 1, 1],\n",
            "        [1, 1, 1, ..., 1, 1, 1],\n",
            "        ...,\n",
            "        [1, 1, 1, ..., 1, 1, 1],\n",
            "        [1, 1, 1, ..., 1, 1, 1],\n",
            "        [1, 1, 1, ..., 1, 1, 1]]], dtype=uint8), False)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#we try sample some data form buffer\n",
        "def sample(batch_size, beta):\n",
        "  samples = {\n",
        "        'weights': np.zeros(shape=batch_size, dtype=np.float32),\n",
        "        'indexes': np.zeros(shape=batch_size, dtype=np.int32)\n",
        "    }\n",
        "  for i in range(batch_size):\n",
        "    samples['indexes'][i]=np.random.choice(32, 1)\n",
        "  prob_min = 0.2/4 #min/sum\n",
        "  max_weight = (prob_min * 32) ** (-beta)#max w\n",
        "  for i in range(batch_size):\n",
        "    idx = samples['indexes'][i]\n",
        "    prob = 0.2/4 #self/sum\n",
        "    weight = (prob * 32) ** (-beta)#weight without max w\n",
        "    samples['weights'][i] = weight / max_weight\n",
        "  for k, v in data.items():\n",
        "    samples[k] = v[samples['indexes']]\n",
        "  return samples\n",
        "batch_size =63\n",
        "beta=0.2\n",
        "samples=sample(batch_size, beta)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D3uVR1JUsYig",
        "outputId": "e8514e39-9873-4ed6-cba3-8fd6845c953f"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-45-3df89bfc82ef>:8: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n",
            "  samples['indexes'][i]=np.random.choice(32, 1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#credit: https://nn.labml.ai/rl/dqn/replay_buffer.html\n",
        "import random\n",
        "import numpy as np\n",
        "\n",
        "class ReplayBuffer:\n",
        "  def __init__(self, capacity, alpha):#alpha high: only cares the important low: all the same\n",
        "    self.capacity = capacity\n",
        "    self.alpha = alpha\n",
        "\n",
        "    self.priority_sum = [0 for _ in range(2 * self.capacity)]#sement tree to get sum of\n",
        "    #float('inf') as an integer to represent it as infinity\n",
        "    self.priority_min = [float('inf') for _ in range(2 * self.capacity)]\n",
        "\n",
        "    #Current max priority, p, to be assigned to new transitions\n",
        "    self.max_priority = 1.#p-value\n",
        "\n",
        "    self.data = {\n",
        "        'obs': np.zeros(shape=(capacity, 4, 84, 84), dtype=np.uint8),\n",
        "        'action': np.zeros(shape=capacity, dtype=np.int32),\n",
        "        'reward': np.zeros(shape=capacity, dtype=np.float32),\n",
        "        'next_obs': np.zeros(shape=(capacity, 4, 84, 84), dtype=np.uint8),\n",
        "        'done': np.zeros(shape=capacity, dtype=np.bool)\n",
        "    }\n",
        "\n",
        "    #cyclic buffers: FIFO\n",
        "    self.next_idx = 0\n",
        "    #Size of the buffer\n",
        "    self.size = 0\n",
        "\n",
        "  def add(self, obs, action, reward, next_obs, done):\n",
        "    idx = self.next_idx #where to put?\n",
        "\n",
        "    #store\n",
        "    self.data['obs'][idx] = obs\n",
        "    self.data['action'][idx] = action\n",
        "    self.data['reward'][idx] = reward\n",
        "    self.data['next_obs'][idx] = next_obs\n",
        "    self.data['done'][idx] = done\n",
        "\n",
        "    #update index of in and out\n",
        "    self.next_idx = (idx + 1) % self.capacity\n",
        "    self.size = min(self.capacity, self.size + 1)\n",
        "\n",
        "    priority_alpha = self.max_priority ** self.alpha\n",
        "    self._set_priority_min(idx, priority_alpha)\n",
        "    self._set_priority_sum(idx, priority_alpha)\n",
        "\n",
        "  #Set priority in binary segment tree and return minimum\n",
        "  def _set_priority_min(self, idx, priority_alpha):\n",
        "    idx += self.capacity\n",
        "    self.priority_min[idx] = priority_alpha\n",
        "\n",
        "    while idx >= 2:\n",
        "      idx //= 2\n",
        "      self.priority_min[idx] = min(self.priority_min[2 * idx], self.priority_min[2 * idx + 1])\n",
        "\n",
        "  #Set priority in binary segment tree and return sum\n",
        "  def _set_priority_sum(self, idx, priority):\n",
        "    idx += self.capacity\n",
        "    self.priority_sum[idx] = priority\n",
        "\n",
        "    while idx >= 2:\n",
        "      idx //= 2\n",
        "      self.priority_sum[idx]= self.priority_sum[2 * idx] + self.priority_sum[2 * idx + 1]\n",
        "\n",
        "  def _sum(self):\n",
        "    return self.priority_sum[1]\n",
        "\n",
        "  def _min(self):\n",
        "    return self.priority_min[1]\n",
        "\n",
        "  #Find i, that p0+...pi >= prefix_sum\n",
        "  def find_prefix_sum_idx(self, prefix_sum):\n",
        "    idx = 0\n",
        "    #find the index on the leaf\n",
        "    while idx < self.capacity:\n",
        "      if self.priority_sum[idx * 2] > prefix_sum:\n",
        "        idx = 2 * idx\n",
        "      else:\n",
        "        prefix_sum -= self.priority_sum[idx * 2]\n",
        "        idx = 2 * idx + 1\n",
        "    return idx - self.capacity\n",
        "\n",
        "  #Sample from buffer\n",
        "  def sample(self, batch_size, beta):\n",
        "    samples = {\n",
        "        'weights': np.zeros(shape=batch_size, dtype=np.float32),\n",
        "        'indexes': np.zeros(shape=batch_size, dtype=np.int32)\n",
        "    }\n",
        "    #set indexes\n",
        "    for i in range(batch_size):\n",
        "      p = random.random() * self._sum()#random priority\n",
        "      idx = self.find_prefix_sum_idx(p)\n",
        "      samples['indexes'][i] = idx\n",
        "     #get the max w\n",
        "    prob_min = self._min() / self._sum()\n",
        "    max_weight = (prob_min * self.size) ** (-beta)#max w\n",
        "\n",
        "    #GET weight\n",
        "    for i in range(batch_size):\n",
        "      idx = samples['indexes'][i]\n",
        "      prob = self.priority_sum[idx + self.capacity] / self._sum()\n",
        "      weight = (prob * self.size) ** (-beta)\n",
        "      samples['weights'][i] = weight / max_weight\n",
        "    #add the rest data in samples\n",
        "    for k, v in self.data.items():\n",
        "      samples[k] = v[samples['indexes']]\n",
        "    return samples\n",
        "\n",
        "    def update_priorities(self, indexes, priorities):\n",
        "      for idx, priority in zip(indexes, priorities):\n",
        "        self.max_priority = max(self.max_priority, priority)\n",
        "        priority_alpha = priority ** self.alpha\n",
        "\n",
        "        self._set_priority_min(idx, priority_alpha)\n",
        "        self._set_priority_sum(idx, priority_alpha)\n",
        "    def is_full(self):\n",
        "      return self.capacity == self.size"
      ],
      "metadata": {
        "id": "31YJ1PvV-LRN"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Deep Q Networks (DQN)"
      ],
      "metadata": {
        "id": "CZwfxp9oD9xO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<p>For now we only got the model,but we still need to loss function to calculate the loss</p>\n",
        "<p>In order to do that,we should get the 'real' q  by using bellman equations, and compare it with model_q_value </p>"
      ],
      "metadata": {
        "id": "oQTrpVw3GQWE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install labml-nn"
      ],
      "metadata": {
        "id": "0NXYmwQKFe3b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "t = torch.tensor([[1, 2], [3, 4]])\n",
        "print(f\"t:   {t}\")\n",
        "#pick elements form row/column and form a new matrix\n",
        "torch.gather(t, -1, torch.tensor([[0, 1,1], [1, 0,1]]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HJ0qMFv7M4y7",
        "outputId": "1af46f48-1bd2-4a4d-bc3e-6c7834da8b5b"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "t:   tensor([[1, 2],\n",
            "        [3, 4]])\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[1, 2, 2],\n",
              "        [4, 3, 4]])"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<p>First get the Q(s,a) from the output of model and corresponding's action</p><p>Second we need the q(s',a),it will be given by double  Q-Learning</p>\n",
        "<p>Third calculate the loss and TD-error</p>"
      ],
      "metadata": {
        "id": "JwncYoPY96sh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "def forward(q: torch.Tensor, action: torch.Tensor, double_q_next: torch.Tensor,\n",
        "              target_q_next: torch.Tensor, done: torch.Tensor, reward: torch.Tensor,\n",
        "              weights: torch.Tensor):\n",
        "  q_action = q.gather(-1, action.to(torch.long).unsqueeze(-1)).squeeze(-1)\n",
        "  with torch.no_grad():\n",
        "    best_next_action = torch.argmax(double_q_next, -1)\n",
        "    best_next_q_value = target_q_next.gather(-1, best_next_action.unsqueeze(-1)).squeeze(-1)\n",
        "    q_update = reward + 0.1 * best_next_q_value * (1 - done)\n",
        "    td_error = q_action - q_update\n",
        "  huber_loss = nn.SmoothL1Loss(reduction='none')\n",
        "  losses = huber_loss(q_action, q_update)\n",
        "  loss = torch.mean(weights * losses)\n",
        "  return loss,td_error\n",
        "\n",
        "\n",
        "q= output\n",
        "action= torch.from_numpy(np.random.choice(4, 32))\n",
        "double_q_next =Model()\n",
        "double_q_next=double_q_next(torch.rand(size=(32,4,84,84)))\n",
        "target_q_next = Model()\n",
        "target_q_next =target_q_next(torch.rand(size=(32,4,84,84)))\n",
        "done=False\n",
        "reward=torch.ones(32)\n",
        "weights = torch.rand(32)\n",
        "forward(q, action, double_q_next,\n",
        "              target_q_next, done, reward,\n",
        "              weights)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "moUnBC_XGMol",
        "outputId": "7cfbc29f-48dd-4130-e478-fc84f0e81c32"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor(0.2524, grad_fn=<MeanBackward0>),\n",
              " tensor([-0.9737, -0.9736, -1.0169, -1.0171, -1.0265, -1.0263, -0.9847, -1.0172,\n",
              "         -1.0169, -0.9736, -0.9849, -1.0266, -0.9851, -0.9849, -0.9737, -0.9733,\n",
              "         -0.9847, -0.9737, -0.9850, -1.0263, -0.9850, -1.0172, -1.0171, -0.9848,\n",
              "         -1.0259, -0.9848, -0.9848, -0.9734, -0.9734, -1.0265, -0.9736, -0.9851]))"
            ]
          },
          "metadata": {},
          "execution_count": 44
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# https://nn.labml.ai/rl/dqn/model.html\n",
        "from typing import Tuple\n",
        "import torch\n",
        "from torch import nn\n",
        "\n",
        "\n",
        "class QFuncLoss(Model):\n",
        "  def __init__(self, gamma: float):\n",
        "    super().__init__()\n",
        "    self.gamma = gamma\n",
        "    self.huber_loss = nn.SmoothL1Loss(reduction='none')\n",
        "  def forward(self, q: torch.Tensor, action: torch.Tensor, double_q: torch.Tensor,\n",
        "              target_q: torch.Tensor, done: torch.Tensor, reward: torch.Tensor,\n",
        "              weights: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor]:\n",
        "\n",
        "              q_sampled_action = q.gather(-1, action.to(torch.long).unsqueeze(-1)).squeeze(-1)\n",
        "              #tracker.add('q_sampled_action', q_sampled_action)\n",
        "              with torch.no_grad():\n",
        "                #use double to choose action, traget uses ations to get Q-value\n",
        "                best_next_action = torch.argmax(double_q, -1)\n",
        "                best_next_q_value = target_q.gather(-1, best_next_action.unsqueeze(-1)).squeeze(-1)\n",
        "\n",
        "                #update\n",
        "                q_update = reward + self.gamma * best_next_q_value * (1 - done)\n",
        "                #tracker.add('q_update', q_update)\n",
        "\n",
        "                #TD\n",
        "                td_error = q_sampled_action - q_update\n",
        "                #tracker.add('td_error', td_error)\n",
        "              losses = self.huber_loss(q_sampled_action, q_update)\n",
        "              loss = torch.mean(weights * losses)\n",
        "              #tracker.add('loss', loss)\n",
        "              return td_error, loss\n"
      ],
      "metadata": {
        "id": "Eo6ugJhKHOPI"
      },
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Game environment\n",
        "\n",
        "Not really understnd it...."
      ],
      "metadata": {
        "id": "DSOm1YtuVgET"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#credit: https://nn.labml.ai/rl/game.html\n",
        "from multiprocessing import Process\n",
        "import os\n",
        "import multiprocessing\n",
        "\n",
        "def info(title):\n",
        "    print(title)\n",
        "    print('module name:', __name__)\n",
        "    print('parent process:', os.getppid())\n",
        "    print('process id:', os.getpid())\n",
        "\n",
        "def f(a):\n",
        "    a=a.recv()\n",
        "    info('function f')\n",
        "    print('hello', \"what's up\",a)\n",
        "\n",
        "child, parent = multiprocessing.Pipe()\n",
        "p = Process(target=f, args=(parent,))\n",
        "child.send('AAHPH')\n",
        "p.start()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6-vTgblJa5P-",
        "outputId": "29d840e2-3f18-4d12-b217-a61dc5c64d79"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "function f\n",
            "module name: __main__\n",
            "parent process: 4228\n",
            "process id: 4343\n",
            "hello what's up \n",
            "AAHPH"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import multiprocessing\n",
        "import multiprocessing.connection\n",
        "\n",
        "import cv2\n",
        "import gym\n",
        "import numpy as np\n",
        "\n",
        "class Game:\n",
        "  def __init__(self, seed: int):\n",
        "    self.env = gym.make('BreakoutNoFrameskip-v4')\n",
        "    self.env.seed(seed)#making experiments reproducible.\n",
        "\n",
        "    self.obs_4 = np.zeros((4, 84, 84))#a stack of 4 frames\n",
        "    self.obs_2_max = np.zeros((2, 84, 84))#buffer to keep the maximum of last 2 frames\n",
        "\n",
        "    self.rewards = []\n",
        "    self.lives = 0\n",
        "  def step(self, action):\n",
        "    reward = 0.\n",
        "    done = None\n",
        "\n",
        "    for i in range(4):\n",
        "      obs, r, done, info = self.env.step(action)\n",
        "      if i >= 2:\n",
        "        self.obs_2_max[i % 2] = self._process_obs(obs)#proceed img\n",
        "      reward += r\n",
        "      lives = self.env.unwrapped.ale.lives()\n",
        "      if lives < self.lives:#if a life is lost\n",
        "        done = True\n",
        "        break\n",
        "    self.rewards.append(reward)\n",
        "\n",
        "    if done:\n",
        "      episode_info = {\"reward\": sum(self.rewards), \"length\": len(self.rewards)}\n",
        "      self.reset()\n",
        "    else:\n",
        "      episode_info = None\n",
        "      obs = self.obs_2_max.max(axis=0)\n",
        "      self.obs_4 = np.roll(self.obs_4, shift=-1, axis=0)#move 1 step left\n",
        "      self.obs_4[-1] = obs#replacce the oldest\n",
        "    return self.obs_4, reward, done, episode_info\n",
        "\n",
        "  def reset(self):\n",
        "    obs = self.env.reset()\n",
        "    #reset caches\n",
        "    obs = self._process_obs(obs)\n",
        "    for i in range(4):\n",
        "      self.obs_4[i] = obs\n",
        "    self.rewards = []\n",
        "    self.lives = self.env.unwrapped.ale.lives()\n",
        "\n",
        "  @staticmethod\n",
        "  def _process_obs(obs):\n",
        "    obs = cv2.cvtColor(obs, cv2.COLOR_RGB2GRAY)\n",
        "    obs = cv2.resize(obs, (84, 84), interpolation=cv2.INTER_AREA)\n",
        "    return obs\n",
        "\n",
        "def worker_process(remote: multiprocessing.connection.Connection, seed: int):\n",
        "\n",
        "  game = Game(seed)\n",
        "  while True:\n",
        "    cmd, data = remote.recv()\n",
        "    if cmd == \"step\":\n",
        "      remote.send(game.step(data))\n",
        "    elif cmd == \"reset\":\n",
        "      remote.send(game.reset())\n",
        "    elif cmd == \"close\":\n",
        "      remote.close()\n",
        "      break\n",
        "    else:\n",
        "      raise NotImplementedError\n",
        "class Worker:\n",
        "  def __init__(self, seed):\n",
        "    self.child, parent = multiprocessing.Pipe()#send--->recieve\n",
        "    self.process = multiprocessing.Process(target=worker_process, args=(parent, seed))\n",
        "    self.process.start()"
      ],
      "metadata": {
        "id": "VBJqeQ12VfIr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# DQN Experiment with Atari Breakout"
      ],
      "metadata": {
        "id": "S6bDrQ6H8g01"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip3 install mxnet-mkl==1.6.0 numpy==1.23.1"
      ],
      "metadata": {
        "id": "v8hcH1aW4s_J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<p>I try to train our model, but i dont really understnd the environment,so i assume i've alredy have the buffer info</p>"
      ],
      "metadata": {
        "id": "qDqWrl0c_GdO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<p></p>"
      ],
      "metadata": {
        "id": "PwRbeVELEC0r"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "def obs_to_torch(obs: np.ndarray) -> torch.Tensor:\n",
        "\n",
        "    return torch.tensor(obs, dtype=torch.float32)\n",
        "EPOCHS=100\n",
        "model = Model()\n",
        "target_model = Model()\n",
        "loss_func = QFuncLoss(0.99)\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=2.5e-4)\n",
        "def train():\n",
        "  for epoch in range(EPOCHS):\n",
        "    model.train()\n",
        "    #q(s,a)\n",
        "    q_value = model(obs_to_torch(samples['obs']))\n",
        "    #q(s',a)\n",
        "    with torch.inference_mode():\n",
        "      double_q_value = model(obs_to_torch(samples['next_obs']))\n",
        "      target_q_value =target_model(obs_to_torch(samples['next_obs']))\n",
        "    #Loss\n",
        "    td_errors, loss =loss_func(q_value,\n",
        "                  q_value.new_tensor(samples['action']),\n",
        "                  double_q_value, target_q_value,\n",
        "                  q_value.new_tensor(samples['done']),\n",
        "                  q_value.new_tensor(samples['reward']),\n",
        "                  q_value.new_tensor(samples['weights']))\n",
        "    new_priorities = td_errors\n",
        "    #clean record\n",
        "    optimizer.zero_grad()\n",
        "    #gradient\n",
        "    loss.backward()\n",
        "    #change the setting\n",
        "    optimizer.step()\n",
        "    if epoch%10==0:\n",
        "      print(f'epoch:{epoch}---train_loss:{loss}')\n",
        "train()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "expv1-2Q_Fi4",
        "outputId": "19b9c3ae-d4f8-4bc1-b675-5515f8691e08"
      },
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch:0---train_loss:31.711091995239258\n",
            "epoch:10---train_loss:31.508651733398438\n",
            "epoch:20---train_loss:27.719820022583008\n",
            "epoch:30---train_loss:6.995792388916016\n",
            "epoch:40---train_loss:6.931807994842529\n",
            "epoch:50---train_loss:4.018191814422607\n",
            "epoch:60---train_loss:0.04735827073454857\n",
            "epoch:70---train_loss:0.4734249413013458\n",
            "epoch:80---train_loss:0.1565019190311432\n",
            "epoch:90---train_loss:0.6465544104576111\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#credit: https://nn.labml.ai/rl/dqn/experiment.html\n",
        "import numpy as np\n",
        "import torch\n",
        "from labml import tracker, experiment, logger, monit\n",
        "from labml.internal.configs.dynamic_hyperparam import FloatDynamicHyperParam\n",
        "from labml_helpers.schedule import Piecewise\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "    device = torch.device(\"cuda:0\")\n",
        "else:\n",
        "    device = torch.device(\"cpu\")\n",
        "def obs_to_torch(obs: np.ndarray) -> torch.Tensor:\n",
        "  return torch.tensor(obs, dtype=torch.float32, device=device) / 255.\n",
        "class Trainer:\n",
        "  def __init__(self, *,updates: int, epochs: int,\n",
        "              n_workers: int, worker_steps: int, mini_batch_size: int,\n",
        "              update_target_model: int,\n",
        "              learning_rate: FloatDynamicHyperParam,):\n",
        "    self.n_workers = n_workers\n",
        "    self.worker_steps = worker_steps\n",
        "    self.train_epochs = epochs\n",
        "    self.updates = updates\n",
        "    self.mini_batch_size = mini_batch_size\n",
        "    self.update_target_model = update_target_model\n",
        "    self.learning_rate = learning_rate\n",
        "    self.exploration_coefficient = Piecewise([\n",
        "               (0, 1.0),\n",
        "               (25_000, 0.1),\n",
        "               (self.updates / 2, 0.01)], outside_value=0.01)\n",
        "    self.prioritized_replay_beta = Piecewise([\n",
        "                (0, 0.4),\n",
        "                (self.updates, 1)], outside_value=1)\n",
        "    self.replay_buffer = ReplayBuffer(2 ** 14, 0.6)\n",
        "    self.model = Model().to(device)\n",
        "    self.target_model = Model().to(device)\n",
        "    self.workers = [Worker(47 + i) for i in range(self.n_workers)]\n",
        "    # initialize tensors for observations\n",
        "    self.obs = np.zeros((self.n_workers, 4, 84, 84), dtype=np.uint8)\n",
        "\n",
        "    # reset the workers\n",
        "    for worker in self.workers:\n",
        "      worker.child.send((\"reset\", None))\n",
        "\n",
        "    # get the initial observations\n",
        "    for i, worker in enumerate(self.workers):\n",
        "      self.obs[i] = worker.child.recv()\n",
        "\n",
        "    # loss function\n",
        "    self.loss_func = QFuncLoss(0.99)\n",
        "    # optimizer\n",
        "    self.optimizer = torch.optim.Adam(self.model.parameters(), lr=2.5e-4)\n",
        "\n",
        "  def _sample_action(self, q_value: torch.Tensor, exploration_coefficient: float):\n",
        "    with torch.no_grad():\n",
        "      greedy_action = torch.argmax(q_value, dim=-1)\n",
        "      random_action = torch.randint(q_value.shape[-1], greedy_action.shape, device=q_value.device)\n",
        "      is_choose_rand = torch.rand(greedy_action.shape, device=q_value.device) < exploration_coefficient\n",
        "      return torch.where(is_choose_rand, random_action, greedy_action).cpu().numpy()\n",
        "\n",
        "  def sample(self, exploration_coefficient: float):\n",
        "    with torch.no_grad():\n",
        "      for t in range(self.worker_steps):\n",
        "        q_value = self.model(obs_to_torch(self.obs))\n",
        "        actions = self._sample_action(q_value, exploration_coefficient)\n",
        "        for w, worker in enumerate(self.workers):\n",
        "          worker.child.send((\"step\", actions[w]))\n",
        "        for w, worker in enumerate(self.workers):\n",
        "          next_obs, reward, done, info = worker.child.recv()\n",
        "          self.replay_buffer.add(self.obs[w], actions[w], reward, next_obs, done)\n",
        "          if info:\n",
        "            tracker.add('reward', info['reward'])\n",
        "            tracker.add('length', info['length'])\n",
        "          self.obs[w] = next_obs\n",
        "  def train(self, beta: float):\n",
        "    for _ in range(self.train_epochs):\n",
        "      samples = self.replay_buffer.sample(self.mini_batch_size, beta)\n",
        "      q_value = self.model(obs_to_torch(samples['obs']))\n",
        "      with torch.no_grad():\n",
        "        double_q_value = self.model(obs_to_torch(samples['next_obs']))\n",
        "        target_q_value = self.target_model(obs_to_torch(samples['next_obs']))\n",
        "      #Compute Temporal Difference (TD) errors, δ, and the loss, L(θ)\n",
        "      td_errors, loss = self.loss_func(q_value,q_value.new_tensor(samples['action']),\n",
        "                         double_q_value, target_q_value,\n",
        "                        q_value.new_tensor(samples['done']),\n",
        "                        q_value.new_tensor(samples['reward']),\n",
        "                        q_value.new_tensor(samples['weights']))\n",
        "      new_priorities = np.abs(td_errors.cpu().numpy()) + 1e-6\n",
        "      self.replay_buffer.update_priorities(samples['indexes'], new_priorities)\n",
        "      for pg in self.optimizer.param_groups:\n",
        "        pg['lr'] = self.learning_rate()\n",
        "\n",
        "      self.optimizer.zero_grad()\n",
        "      loss.backward()\n",
        "      torch.nn.utils.clip_grad_norm_(self.model.parameters(), max_norm=0.5)\n",
        "      self.optimizer.step()\n",
        "\n",
        "    def run_training_loop(self):\n",
        "       tracker.set_queue('reward', 100, True)\n",
        "       tracker.set_queue('length', 100, True)\n",
        "\n",
        "       self.target_model.load_state_dict(self.model.state_dict())\n",
        "\n",
        "       for update in monit.loop(self.updates):\n",
        "        exploration = self.exploration_coefficient(update)\n",
        "        tracker.add('exploration', exploration)\n",
        "\n",
        "        beta = self.prioritized_replay_beta(update)\n",
        "        tracker.add('beta', beta)\n",
        "        self.sample(exploration)\n",
        "\n",
        "        if self.replay_buffer.is_full():\n",
        "          self.train(beta)\n",
        "          if update % self.update_target_model == 0:\n",
        "            self.target_model.load_state_dict(self.model.state_dict())\n",
        "        tracker.save()\n",
        "        if (update + 1) % 1_000 == 0:\n",
        "          logger.log()\n",
        "    def destroy(self):\n",
        "        \"\"\"\n",
        "        ### Destroy\n",
        "        Stop the workers\n",
        "        \"\"\"\n",
        "        for worker in self.workers:\n",
        "            worker.child.send((\"close\", None))\n",
        "\n",
        "\n",
        "def main():\n",
        "\n",
        "  # Create the experiment\n",
        "  experiment.create(name='dqn')\n",
        "\n",
        "  # Configurations\n",
        "  configs = {\n",
        "        # Number of updates\n",
        "        'updates': 1_000_000,\n",
        "        # Number of epochs to train the model with sampled data.\n",
        "        'epochs': 8,\n",
        "        # Number of worker processes\n",
        "        'n_workers': 8,\n",
        "        # Number of steps to run on each process for a single update\n",
        "        'worker_steps': 4,\n",
        "        # Mini batch size\n",
        "        'mini_batch_size': 32,\n",
        "        # Target model updating interval\n",
        "        'update_target_model': 250,\n",
        "        # Learning rate.\n",
        "        'learning_rate': FloatDynamicHyperParam(1e-4, (0, 1e-3)),\n",
        "    }\n",
        "\n",
        "  # Configurations\n",
        "  experiment.configs(configs)\n",
        "\n",
        "  # Initialize the trainer\n",
        "  m = Trainer(**configs)\n",
        "  # Run and monitor the experiment\n",
        "  with experiment.start():\n",
        "    m.run_training_loop()\n",
        "  # Stop the workers\n",
        "  m.destroy()\n",
        "\n",
        "\n",
        "# ## Run it\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "8S-FJ8788f4g",
        "outputId": "bbdc2341-e8a7-4f49-8b53-1b5d106484b3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<pre style=\"overflow-x: scroll;\"></pre>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-3-a4a2a07a63f4>:21: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.\n",
            "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
            "  'done': np.zeros(shape=capacity, dtype=np.bool)\n",
            "Process Process-10:\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 314, in _bootstrap\n",
            "    self.run()\n",
            "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 108, in run\n",
            "    self._target(*self._args, **self._kwargs)\n",
            "Process Process-12:\n",
            "Process Process-11:\n",
            "  File \"<ipython-input-8-af899276e9fd>\", line 60, in worker_process\n",
            "    game = Game(seed)\n",
            "Traceback (most recent call last):\n",
            "Traceback (most recent call last):\n",
            "Process Process-13:\n",
            "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 314, in _bootstrap\n",
            "    self.run()\n",
            "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 314, in _bootstrap\n",
            "    self.run()\n",
            "  File \"<ipython-input-8-af899276e9fd>\", line 10, in __init__\n",
            "    self.env = gym.make('BreakoutNoFrameskip-v4')\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/gym/envs/registration.py\", line 607, in make\n",
            "    _check_version_exists(ns, name, version)\n",
            "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 108, in run\n",
            "    self._target(*self._args, **self._kwargs)\n",
            "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 314, in _bootstrap\n",
            "    self.run()\n",
            "  File \"<ipython-input-8-af899276e9fd>\", line 60, in worker_process\n",
            "    game = Game(seed)\n",
            "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 108, in run\n",
            "    self._target(*self._args, **self._kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/gym/envs/registration.py\", line 234, in _check_version_exists\n",
            "    _check_name_exists(ns, name)\n",
            "Process Process-14:\n",
            "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 108, in run\n",
            "    self._target(*self._args, **self._kwargs)\n",
            "  File \"<ipython-input-8-af899276e9fd>\", line 60, in worker_process\n",
            "    game = Game(seed)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/gym/envs/registration.py\", line 212, in _check_name_exists\n",
            "    raise error.NameNotFound(\n",
            "  File \"<ipython-input-8-af899276e9fd>\", line 10, in __init__\n",
            "    self.env = gym.make('BreakoutNoFrameskip-v4')\n",
            "Traceback (most recent call last):\n",
            "Process Process-15:\n",
            "  File \"<ipython-input-8-af899276e9fd>\", line 60, in worker_process\n",
            "    game = Game(seed)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/gym/envs/registration.py\", line 607, in make\n",
            "    _check_version_exists(ns, name, version)\n",
            "  File \"<ipython-input-8-af899276e9fd>\", line 10, in __init__\n",
            "    self.env = gym.make('BreakoutNoFrameskip-v4')\n",
            "gym.error.NameNotFound: Environment BreakoutNoFrameskip doesn't exist. \n",
            "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 314, in _bootstrap\n",
            "    self.run()\n",
            "Process Process-16:\n",
            "  File \"<ipython-input-8-af899276e9fd>\", line 60, in worker_process\n",
            "    game = Game(seed)\n",
            "  File \"<ipython-input-8-af899276e9fd>\", line 10, in __init__\n",
            "    self.env = gym.make('BreakoutNoFrameskip-v4')\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/gym/envs/registration.py\", line 607, in make\n",
            "    _check_version_exists(ns, name, version)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/gym/envs/registration.py\", line 234, in _check_version_exists\n",
            "    _check_name_exists(ns, name)\n",
            "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 108, in run\n",
            "    self._target(*self._args, **self._kwargs)\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/gym/envs/registration.py\", line 212, in _check_name_exists\n",
            "    raise error.NameNotFound(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/gym/envs/registration.py\", line 234, in _check_version_exists\n",
            "    _check_name_exists(ns, name)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/gym/envs/registration.py\", line 607, in make\n",
            "    _check_version_exists(ns, name, version)\n",
            "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 314, in _bootstrap\n",
            "    self.run()\n",
            "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 108, in run\n",
            "    self._target(*self._args, **self._kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/gym/envs/registration.py\", line 234, in _check_version_exists\n",
            "    _check_name_exists(ns, name)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/gym/envs/registration.py\", line 212, in _check_name_exists\n",
            "    raise error.NameNotFound(\n",
            "  File \"<ipython-input-8-af899276e9fd>\", line 10, in __init__\n",
            "    self.env = gym.make('BreakoutNoFrameskip-v4')\n",
            "gym.error.NameNotFound: Environment BreakoutNoFrameskip doesn't exist. \n",
            "  File \"<ipython-input-8-af899276e9fd>\", line 60, in worker_process\n",
            "    game = Game(seed)\n",
            "  File \"<ipython-input-8-af899276e9fd>\", line 10, in __init__\n",
            "    self.env = gym.make('BreakoutNoFrameskip-v4')\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/gym/envs/registration.py\", line 607, in make\n",
            "    _check_version_exists(ns, name, version)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/gym/envs/registration.py\", line 234, in _check_version_exists\n",
            "    _check_name_exists(ns, name)\n",
            "gym.error.NameNotFound: Environment BreakoutNoFrameskip doesn't exist. \n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/gym/envs/registration.py\", line 607, in make\n",
            "    _check_version_exists(ns, name, version)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/gym/envs/registration.py\", line 212, in _check_name_exists\n",
            "    raise error.NameNotFound(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/gym/envs/registration.py\", line 212, in _check_name_exists\n",
            "    raise error.NameNotFound(\n",
            "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 314, in _bootstrap\n",
            "    self.run()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/gym/envs/registration.py\", line 234, in _check_version_exists\n",
            "    _check_name_exists(ns, name)\n",
            "gym.error.NameNotFound: Environment BreakoutNoFrameskip doesn't exist. \n",
            "gym.error.NameNotFound: Environment BreakoutNoFrameskip doesn't exist. \n",
            "Process Process-17:\n",
            "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 108, in run\n",
            "    self._target(*self._args, **self._kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/gym/envs/registration.py\", line 212, in _check_name_exists\n",
            "    raise error.NameNotFound(\n",
            "  File \"<ipython-input-8-af899276e9fd>\", line 60, in worker_process\n",
            "    game = Game(seed)\n",
            "gym.error.NameNotFound: Environment BreakoutNoFrameskip doesn't exist. \n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 314, in _bootstrap\n",
            "    self.run()\n",
            "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 108, in run\n",
            "    self._target(*self._args, **self._kwargs)\n",
            "  File \"<ipython-input-8-af899276e9fd>\", line 10, in __init__\n",
            "    self.env = gym.make('BreakoutNoFrameskip-v4')\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/gym/envs/registration.py\", line 607, in make\n",
            "    _check_version_exists(ns, name, version)\n",
            "  File \"<ipython-input-8-af899276e9fd>\", line 60, in worker_process\n",
            "    game = Game(seed)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/gym/envs/registration.py\", line 234, in _check_version_exists\n",
            "    _check_name_exists(ns, name)\n",
            "  File \"<ipython-input-8-af899276e9fd>\", line 10, in __init__\n",
            "    self.env = gym.make('BreakoutNoFrameskip-v4')\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/gym/envs/registration.py\", line 212, in _check_name_exists\n",
            "    raise error.NameNotFound(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/gym/envs/registration.py\", line 607, in make\n",
            "    _check_version_exists(ns, name, version)\n",
            "gym.error.NameNotFound: Environment BreakoutNoFrameskip doesn't exist. \n",
            "  File \"/usr/local/lib/python3.10/dist-packages/gym/envs/registration.py\", line 234, in _check_version_exists\n",
            "    _check_name_exists(ns, name)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/gym/envs/registration.py\", line 212, in _check_name_exists\n",
            "    raise error.NameNotFound(\n",
            "gym.error.NameNotFound: Environment BreakoutNoFrameskip doesn't exist. \n"
          ]
        },
        {
          "output_type": "error",
          "ename": "BrokenPipeError",
          "evalue": "[Errno 32] Broken pipe",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mBrokenPipeError\u001b[0m                           Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-11-a285f28ecb1b>\u001b[0m in \u001b[0;36m<cell line: 162>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    161\u001b[0m \u001b[0;31m# ## Run it\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    162\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 163\u001b[0;31m     \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    164\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    165\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-11-a285f28ecb1b>\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    151\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m   \u001b[0;31m# Initialize the trainer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 153\u001b[0;31m   \u001b[0mm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTrainer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mconfigs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    154\u001b[0m   \u001b[0;31m# Run and monitor the experiment\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    155\u001b[0m   \u001b[0;32mwith\u001b[0m \u001b[0mexperiment\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-11-a285f28ecb1b>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, updates, epochs, n_workers, worker_steps, mini_batch_size, update_target_model, learning_rate)\u001b[0m\n\u001b[1;32m     39\u001b[0m     \u001b[0;31m# reset the workers\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mworker\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mworkers\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 41\u001b[0;31m       \u001b[0mworker\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchild\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"reset\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     42\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m     \u001b[0;31m# get the initial observations\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/multiprocessing/connection.py\u001b[0m in \u001b[0;36msend\u001b[0;34m(self, obj)\u001b[0m\n\u001b[1;32m    204\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_closed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    205\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_writable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 206\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_send_bytes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_ForkingPickler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdumps\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    207\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    208\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mrecv_bytes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmaxlength\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/multiprocessing/connection.py\u001b[0m in \u001b[0;36m_send_bytes\u001b[0;34m(self, buf)\u001b[0m\n\u001b[1;32m    409\u001b[0m                 \u001b[0;31m# Also note we want to avoid sending a 0-length buffer separately,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    410\u001b[0m                 \u001b[0;31m# to avoid \"broken pipe\" errors if the other end closed the pipe.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 411\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_send\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mheader\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mbuf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    412\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    413\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_recv_bytes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmaxsize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/multiprocessing/connection.py\u001b[0m in \u001b[0;36m_send\u001b[0;34m(self, buf, write)\u001b[0m\n\u001b[1;32m    366\u001b[0m         \u001b[0mremaining\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbuf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    367\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 368\u001b[0;31m             \u001b[0mn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    369\u001b[0m             \u001b[0mremaining\u001b[0m \u001b[0;34m-=\u001b[0m \u001b[0mn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    370\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mremaining\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mBrokenPipeError\u001b[0m: [Errno 32] Broken pipe"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "\n",
        "class LinearModel(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.layer1 = nn.Linear(10, 20)\n",
        "        self.layer2 = nn.Linear(20, 3)\n",
        "        self.layer3 = nn.Linear(3, 1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.layer3(self.layer2(self.layer1(x)))\n",
        "\n",
        "\n",
        "\n",
        "model = LinearModel()\n",
        "optimizer = optim.SGD(model.parameters(),0.02)\n",
        "for lr in optimizer.param_groups:\n",
        "  lr['lr'] = 1111\n",
        "optimizer.param_groups"
      ],
      "metadata": {
        "id": "fdOdHoglzO3A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def sample_action( q_value: torch.Tensor, exploration_coefficient: float):\n",
        "  with torch.no_grad():\n",
        "    greedy_action = torch.argmax(q_value, dim=-1)\n",
        "    random_action = torch.randint(q_value.shape[-1], greedy_action.shape, device=q_value.device)\n",
        "    is_choose_rand = torch.rand(greedy_action.shape, device=q_value.device) < exploration_coefficient\n",
        "    return torch.where(is_choose_rand, random_action, greedy_action).cpu().numpy()\n",
        "a=torch.tensor(\n",
        "    [1,2,3,4])\n",
        "sample_action(a,2.2)"
      ],
      "metadata": {
        "id": "xt4ID-JtRXg0"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}